chap 7.  쿠버네티스의 기본 구조
	- 쿠버네티스는 도커보다 규모가 커서 좀 더 빡세다

----------------------------------------------------------------------------------------------


클러스터. 노드. 클러스터링
	: 클러스터링 >> 클러스터를 만드는 것.
	: 클러스터 >> 노드들의 집합.

	: 클러스터는 다양한 분야에서 사용되는 개념으로, 어떤 분야에서 사용되느냐 따라 (클러스터를 구성하는) "노드" 라는 용어가 의미하는 대상이 달라질 수 있다
		: ex) 네트워크 클러스터링 >>
			노드 : 네트워크 장비
			클러스터 : 네트워크 장비의 집합

		: ex) etcd 클러스터링 >>
			노드 : etcd 프로세스를 실행하는 서버
			클러스터 : (etcd 프로세스를 실행하는) 서버들의 집합



에이전트 agent >> 특정 작업을 수행하거나, 다른 시스템/소프트웨어와 상호작용하는 소프트웨어 
	: agent 는 다양한 분야에서 사용되는 개념으로, 어떤 분야에서 사용되느냐 따라 agent의 설치 위치와 구체적인 실행 내용이 다를 수 있다
		: SNMP 프로토콜 의 agent >> 네트워크 장치에서 실행되며  관리 시스템(NMS)과 통신하고, 장치의 상태를 모니터링 및 보고
		: 쿠버네티스 kubelete 의 agent >>쿠버네티스 노드에서 실행되며 API 서버와 통신하고 , 파드(Pod)를 관리하고 및 클러스터의 상태를 보고


Bare Metal 베어 메탈
	: 하드웨어 상에 어떤 소프트웨어도 설치되지 않은 썡 상태
	: 베어 메탈에 운영체제가 설치 된 것 == 우리가 아는 일반적인 컴퓨터



가상화 virtualiztion >> 하나의 리소스(물리적인 하드웨어)를 논리적으로 여러 리소스로 분할하는 기술
	: 하나의 하드웨어에서 여러개의 시스템을 효율적으로 사용할 수 있게 해준다
	: 가상화에서, 실제 하드웨어 == host 이고 가상머신 == Guest 이다
	: 종류
	   : 쉽게 정리하자면 , OS 수준까지 분리 할건지 VS  host OS는 공유하면서 프로세스만 분리할 것인지
		1. 가상머신 vm 을 통한 가상화 
		    : 가상머신 >> 하이퍼바이저를 이용해, 리소스 "전체"를 가상화한 것.
			: 가상머신들은 각각 다른 OS 를 가질 수 있다.  거의 물리적인 컴퓨터와 같은 기능을 한다
			: 하이퍼바이저 hypervisor >> 가상 머신을 생성 및 활용가능하게 하는 소프트웨어
				: VM과 하드웨어간의 IO명령을 처리하는 인터페이스다 >> 생성한 가상머신의 명령어를 (물리적인) 하드웨어가 이해할 수 있도록 번역해주어, 가상머신을 사용 가능하게 해준다
					: os 별로 사용하는 명령어가 다르기 때문에, 가상머신과 하드웨어의 os 가 다르다고 할 때, 그냥 가상머신의 명령어를 그대로 (물리적인) 하드웨어에서 실행하면 해당 하드웨어는 이게 뭔 개소린지 못알아먹는다 
					: 가상머신을 생성할 때 뿐 아니라, 생성한 가상머신을 사용만 할 때에도 반드시 하이퍼바이저를 거쳐야된다.

				: 참고로  가상머신 vm 통한 가상화 방식도 세부 종류 
				    :  일반적으로 가상머신을 통한 가상화라고 하면 호스트 가상화
					(1) 호스트 가상화 >> host os 위에 올라가는 하이퍼바이저
					(2) 하이퍼바이저 가상화 >> host(하드웨어)위 에 바로 올라가는 하이퍼바이저 (host os 가 따로 없다)


			: 가상머신을 사용한 가상화를 할 경우 용량도 크고 속도도 느리다
		

		2. 컨테이너 container 를 통한 가상화
		    : 컨테이너 >> 필요한 프로세스를 표준화된 방식으로 패키징해 격리해 둔 것.
			: 프로세스만 격리한 것으로, 각 컨테이너는  host OS를 공유하게 된다.
				: host OS 를 공유한다
				== 커널을 공유한다 >> 호스트 OS 의 기능을 모두 사용 가능하다
				== 서로 다른 컨테이너끼리 통신을 주고 받을 수 있다
			: 하드웨어위에 host os 가 올라가고, 그 위에 docker 와 같은 컨테이너 응용 프로그램이 올라가고, 그 프로그램 위에 컨테이너들이 올라간다
			:  (걍 화물선에 무작위로 적재하는게 아니라) 표준화된 방식으로 패키징(컨테이너 박스)해서, 컨테이너 엔진(표준화된 운송선)만 있으면 돌아갈 수 있게하는거다.
			: 하이퍼바이저와 guest os 를 따로 필요로 하지 않아 상대적으로 가볍고 빠르다	
			: 컨테이너는 마이크로서비스아키텍쳐 MSA 의 기반이 된다. 
	: https://worlf.tistory.com/141






Container 기술
	: 왜 씀?
		1. 각 애플리케이션을 격리하여 종속성 충돌 문제를 방지
			: 베어메탈(쌩 컴퓨터. 가상머신 같은거 없는) 환경에서는 애플리케이션을 호스트 운영체제에 직접 설치하고 구성하여 애플리케이션 간 종속성 충돌 문제 발생 가능

		2. 가상화 기술을 사용하여 호스트 시스템 리소스를 효율적으로 공유 가능
			: host OS 위에 애플리케이션 프로세스의 형태로 격리한거니까

	: (Linux 운영 체제의 핵심 기능인) Linux namespaces와 cgroups를 기반으로 함
	: (컨테이너 관리를 위한) 주요 컨테이너 도구 >> Docker 등
	: Container 의 역사
		: 1979년, 컨테이너 개념 등장.  ‘chroot’ 시스템 호출의 도입.
			  chroot >> 프로세스와 그 자식 프로세스의 루트 디렉터리를 파일 시스템 내에서 새로운 위치로 변경하는 것으로, 프로세스 격리(Isolation)의 시작

		: 2000년, FreeBSD Jails가 개발
			FreeBSD Jails >> FreeBSD 시스템을 여러 독립적인 작은 “감옥(Jails)”으로 분할하고 각각에게 IP 주소를 할당

		: 2001년, Jail 메커니즘인 Linux VServer가 개발
			Linux VServer >> FreeBSD Jails와 유사한 원리로, 컴퓨터 시스템의 리소스를 분할하는 것을 가능하게함.  Linux 커널을 패치하여 구현됨

		: 2006년, 구글에서 Process Containers개발
			:  Process Containers >> 프로세스 집합의 리소스 사용량(CPU, 메모리, 디스크 I/O, 네트워크)을 제한하고 격리하기 위해 설계됨
				: 2007년에는  Process Containers가 ‘Control Groups(cgroups)’로 이름이 바뀌었으며, Linux 커널 2.6.24에 통합됨

		: 2008년, LXC 가 개발됨
			LXC (LinuX Containers) >> cgroups와 Linux namespaces를 사용하여 구현한 최초의 Linux 컨테이너 엔진으로, Linux 커널을 패치하지 않고도 동작

		: 2013년, LMCTFY(Let Me Contain That For You)가 시작
			LMCTFY(Let Me Contain That For You) >> Linux 애플리케이션 컨테이너를 제공하는 구글 컨테이너 스택의 오픈 소스 버전. 애플리케이션은 “컨테이너 인식”을 통해 하위 컨테이너를 만들고 관리 가능. 
				: 2015년부터 LMCTFY의 핵심 개념은 Open Container Foundation의 일부인 libcontainer로 이전시키기 시작됬으며, 이후 LMCTFY의 배포는 중단됨


		: 2013 년, Docker 가 등장하며 컨테이너 기술이 큰 인기를 얻음
			: 초기 Docker 는 LXC를 기반으로 했지만, 나중엔 이를 자체 라이브러리인 libcontainer 로 교체함
				: LXC 와 libcontainer 
					: LXC >> 전체 가상화 기반. 주로 서버 환경에서 사용됨. 명령줄 도구를 통해 컨테이너를 관리함
					: libcontainer >> OS 수준의 컨테이너화를 가능하게 하는 라이브러리. 주로 간단한 컨테이너 관리를 위해 Docker 와 같은 컨테이너 관리 도구에 내장되어 활용됨



클라우드 >> 가상의 온프레미스. 가상 서버환경.








Docker Network 복습	
    : 관련 지식
	: 가상화 환경(버추얼박스)에서 "게스트"와 "호스트"
		: 호스트 >> 실제 물리적 하드웨어(서버나 시스템)
		: 게스트 >> 호스트 위에서 실행되는 "가상머신"


	: 특수 IP 대역, 사설 IP 대역, 공인 IP 대역
		: 특수 IP 대역 
			(1) 0.0.0.0: 현재 네트워크
			(2) 127.0.0.0 ~ 127.255.255.255: 루프백(Loopback) 주소
			(3) 224.0.0.0 ~ 239.255.255.255: 멀티캐스트(Multicast) 주소
			(4) 240.0.0.0 ~ 255.255.255.255: 연구 및 실험 목적으로 예약된 대역

		: 사설 IP 대역 
			(1) 10.0.0.0 ~ 10.255.255.255 (Class A 의 일부분)
			(2) 172.16.0.0 ~ 172.31.255.255 (Class B의 일부분)
			(3) 192.168.0.0 ~ 192.168.255.255 (Class C의 일부분)

		: 공인 IP 대역 >> "특수IP 대역"과 "사설 IP 대역"을 제외한 나머지 모든 IP 대역



	: NIC Network Interface Card>> 컴퓨터 등의 기기가 인터넷에 연결되게끔 도와주는 부품.랜카드 라고도 함.
		: 기기의 (공인/사설)IP가 여기에 저장되는 거다.

	: MAC 주소 >> NIC 를 가진 단말에게 부여되는 주민등록번호와 같은 고유한, 물리적인 주소
		: LAN 에서는 IP 주소를 MAC 주소에 매칭하는 방식으로 통신이 이루어진다
			: 그러니까 목적지는 IP 주소값으로 지정하지만, 실제론 IP주소값에 바인딩된 MAC 주소 값을 기반으로 타겟 단말을 찾게 된다는 것

		: IP 주소는 논리적이고 가변적이지만 MAC 주소는 물리적이고 불변적이다.
			: IP 주소의 사용 없이 고유한 MAC 주소만을 일일이 라우팅 테이블에 등록했다간 그 값이 ㅈㄴ 많아져서 라우터가 다운되고 말 것.


	: ARP Address Resoultion Protocol >> IP 주소를 MAC 주소와 매칭시키기 위한 프로토콜
	    : ARP 통신 과정
		1. ARP Request >> (LAN 구간의 모든 호스트에게 전달해야하므로) Broadcast 통신으로 해당 IP를 가지는 MAC 주소 정보를 요청하는 패킷을 보낸다.
		2. ARP Response >> ARP Request 를 받은 호스트가 해당 IP를 가지고 있으면 자신의 MAC 주소를 담은 패킷을 (요청한 특정 호스트에게만 전달하면 되므로) Unicast 로  응답한다.
			: ARP Request 받았는데 해당 IP가 자신의 IP가 아닌 경우 해당 호스트는 그 패킷을 걍 읽씹하는거임
			: 그런데 매번 요청마다 이 과정을 반복하는건 좀 비효율적이므로, 한번 ARP 를 통해 얻어졌던 IP 주소 & MAC 주소 정보들은 "ARP Table "로 저장해둔다.
				: ARP table . MAC table .>> IP 주소와 MAC 주소를 1:1 매칭시킨 정보가 정리된 테이블



	: NAT  Network Address Translation  >> 사설 IP 주소와 공인 IP 주소를 변환해주는 "기능".
		: 특정 한 장비를 지칭하는게 아니라 "기능" 인거임
		: 주로 안에서 밖으로 나가는, 사설 IP 를 공인 IP 화 하는 것을 의미
			: 공인 IP를 사설 IP화 하는것은 주로 port forwarding 



	: port forwarding  >> 포트를 활용하여 , 외부망에서 내부망의 특정 PC를 찾아갈 수 있도록 해주는 것. "공인아이피:포트번호" 와 "사설아이피:포트번호"를 매핑해주는 것.
		: 내부망에서 게이트웨이를 통해 외부망으로 나가는건 쉽지만, 외부망에서 내부망으로 들어오는건 쉽지 않은데 이를 가능하게 하는 것이 포트 포워딩이다.
			: 외부망에서 내부망으로 들어오는게 쉽지 않은 이유 >> 사설 IP는 공인IP 처럼 특정 기기를 가리키는게아니라, 여기저기서 사용되기 때문.


	: gateway 
		: gateway 의 두가지 의미
			(1) 출입구/통로의 개념 >>한 네트워크에서 다른 대역의 네트워크로 갈 때 반드시 거쳐가야하는 통로, 출입구. 
				: 보통 gateway 라 하면 이 맥락으로 쓰인다.
				: 이때 gateway 는 주로 IP 주소를 의미한다
					: gateway 가 IP 주소를 의미하는 경우, 라우터의 (공인IP 와 사설 IP 중) 사설 IP 를 의미
						: 출입구니까


			(2) 출입구/통로의 개념이 적용된 네트워크 기기를 포괄하는 용어
				: router ,  방화벽 같은 gateway 개념이 적용된 기기를 gateway 기기 라고도 한다

	: Repeater , Bridge , router
		: https://tomyself148.tistory.com/44
		: 리피터 Repeater >> (전송 거리가 길어서 .. ) 감쇄된 신호를 증폭시키는 장치 
			: 물리 계층(OSI 1계층)에서 동작

		: 브리지 Bridge  >>  Collision Domain을 나누어주는 역할을 하는 장비/소프트웨어. 
			: 즉, 구역을 나누고 통신을 위해 다리를 놓아주는 얘라고 보면 된다.
			: 충돌 도메인 Collision Domain >>데이터가 충돌하는 공간
				: 그러니까 일방통행도로에서 양방향으로 자동차가 진입해 부딪히는 공간 느낌
				: LAN(근거리통신망)에서 연결된 장비가 많으면 많을수록  Collision Domain 이 많아진다
			: 라우터처럼 LAN 과 WAN 을 연결하진 못하고, 브리지는 하나의 네트워크 내부에서 동작하는 얘로, LAN 과 LAN 만(동일한 대역의 네트워크 세그먼트 간의) 연결 가능. 
			: 데이터 링크 계층(OSI 2계층)에서 동작
				:  MAC(Media Access Control) 주소를 기반으로 전송할 포트를 결정하게 된다.

	
		: 라우터 router  >> 내부 네트워크와 외부 네트워크 연결 및 최적 경로(route) 결정해주는 장비
			: 라우터는 내부 ip 주소와 외부 ip 주소를 모두 가지며, 내부 ip 주소가 gateway 에 해당 
				: 참고로 내부 네트워크와 연결되는 IP 즉 gateway 는 Ethernet Interface, 외부 네트워크와 연결되는 IP는 Serial Interface 라고 한다.

			: LAN과 LAN을 연결하거나 LAN과 WAN을 연결해준다.
			: 공유기 가 대표적인 라우터.
			: routing >> 적절한 네트워크로 패킷을 전송하는 것





    : 도커 네트워크 >>  "호스트 머신 내부서 컨테이너 간" || "컨테이너와 외부 간" 자원을 공유할 수 있게 하는 것
	: https://rimo.tistory.com/27
	:  VirtualBox는 "가상화된 라우터", VirtualBox로 생성된 리소스(가상 머신들)는 "그 라우터에 연결된 내부 네트워크 자원들"과 비슷한 개념
		: VirtualBox는 가상 머신들이 속한 네트워크를 관리
		: VirtaulBox 로 생성된, 동일한 대역( 10.X.X.X )의 가상머신들은 같은 네트워크에 있는 것으로 간주된다
			: VirtualBox 와 같은 물리적 컴퓨터에 속하면서 10.X.X.X 의 사설 IP를 가지는 리소스라도, VirtualBox 로 생성된게 아니면 , 해당 리소스는 virtual box 로 생성된 10.X.X.X 대역의 가상 머신과 같은 대역에 있는 것으로 간주되지 않는다
				: 이경우 외부에 있는 리소스로 간주되기 떄문에 Virtual Box 프로그램 자체의 포트 포워딩 설정 등을 활용해야된다 
				: 사설 IP 자체의 대역은 같아도, 같은 공유기에 속하지 않은 경우 사실상 같은 네트워크 대역에 있는 것은 아니기 때문에...
	
			: VirtualBox 로 생성된 리소스(가상머신)들은 동일한 대역에 있는 것으로 간주되므로, 일반적으로 서로간의 통신이 가능하다
				: Virtual Box 프로그램 자체의 네트워크 설정을 통해 (같은 대역임에도) 서로간의 통신이 불가하게 설정 할 수 있다



	: 네트워크 드라이버 >> 일종의 모드라고 봄 됨.
		1. bridge >> 컨테이너에 임의의 새로운 내부 IP 를 할당하여 네트워크를 구성하는 모드
			: 이게 디폴트다.
			: 도커의 bridge 네트워크 드라이버는,  네트워크 분야의 bridge 개념을 바탕으로 설계되었다.
				:  컨테이너들이 서로 통신할 수 있도록, 같은 호스트 내에서 네트워크 세그먼트를 	연결하는 역할을 한다

					:  네트워크 분야의 브리지 >>  네트워크 세그먼트(동일한 네트워크 상의 콜리전 도메인)를 연결하여  물리적 네트워크 장치 간의 트래픽을 관리

					: 도커의 bridge 네트워크 드라이버 >>   네트워크 세그먼트(동일한 도커 호스트 내 컨테이너의 가상 네트워크 인터페이스 )를 연결하여 컨테이너 간의 네트워크 통신을 관리
						:  가상 네트워크 인터페이스 >>  컨테이너들이 동일한 (대역의) 네트워크에 있는 것처럼 상호 작용할 수 있게 해준다.

						: 전통적인 네트워크 브리지의 개념을 가상화 한 것이라 보면 된다


			: docker run 으로 컨테이너를 생성할 때 연결될 브리지를 지정 가능
				docker run -it --name 컨테이너명 --net 브리지명 이미지명
				: 특정 브리지를 명시해주지 않으면 기본적으로 docker0 브리지에 연결된다. 
	
			: 새로운 브리지를 생성할 수도 있다.
				: how >> docker network create --driver bridge 새브리지명
				: 브리지 생성시 subnet >> /16
					: 참고 ) 기본적으로 존재하는 bridge의 subnet >> 172.17.0.0/16

			: 생성한브리지 상세정보출력
				docker inspect 브리지명

			: 같은 브리지를 공유하는 컨테이너끼리는 통신이 쉽게 가능하고(같은 대역대의 통신), 다른 브리지에 속한 컨테이너끼리는 그냥은 불가하다.
			

			: 컨테이너는 각자 독립된 네트워크 공간을 할당받는다.
				: 각 브리지 내의 컨테이너마다 "내부IP"가 도커에 의해 순차적으로 할당됨
					: 다른 브리지의 컨테이너들은 대역대가 다름
					: 내부 IP는 고정이 아님. 재실행할 때마다 바뀔 수 있음
				: 내부 IP만으로는 외부와의 통신이 불가하고, 이를 위해 인터페이스들이 필요한 것
 				: inspect 헸을 떄 나온 "IPAddress" 할당된 내부 IP다.



		2. host >> 컨테이너에 새로운 내부 IP 가 아닌, host 컴퓨터와 동일한 네트워크 환경을 사용하게 하는 모드
			: 결국 호스트 내에서 애플리케이션을 실행한 것과 같아진다.

		3. none >> 아무런 네트워크를 사용하지 않는 모드. 어떠한 외부와도 연결할 필요가 없을 때.



	: 핵심 인터페이스	
	    : 컨테이너가 생성될 때마다, pair interface || peer interface 가 함께 생성된다
		: pair interface , peer interface >>  eth0 이라는 인터페이스와 vethXXXX라는 인터페이스의 쌍을 일컫는 것으로, eth0은 컨테이너 내부에 생성되지만 vethXXXX는 호스트영역에 생성됨을 주의.


	    : 호스트(==우리 )<< 걍 일반사용자권한에서 ifconfig 함 확인 가능
		-1. enp0s3 , eth0 >> 호스트의 NIC. 호스트가 외부와 통신할 수 있게 해준다.
			: 주의 >> 호스트의 NIC로써 호스트가 외부와 통신할 수 있게 해주긴 하지만 공인 IP가 아닌 사설IP가 할당되있는 것으로, 해당 가상머신과 같은 네트워크에 있는 대상에 한해 해당 IP로 통신이 가능하다 
				: 브라우저와 같이 외부 네트워크에 있는 대상의 경우, 통신을 하려면 virtual box 자체의 포트포워딩 설정을 해주거나 걍 로컬호스트로 request 를 해야됨.

			: 옛날엔 eth0, eth1 이런 식으로 썼는데 요즘엔 순서 예측등이 어렵다는 이유로 enp0s3등으로 쓴다.
			: 컨테이너의 eth0 과는 다른거임 주의
			: https://wjddn407.tistory.com/27

		0. vethXXXX >> 각 컨테이너를 시작할 때마다 도커엔진이 "호스트 쪽"에 자동으로 생성시켜주는 "가상 네트워크 인터페이스"로, 각 컨테이너에 외부와의 네트워크를 제공하기 위함.
			: Virtual ETHernet 
			: veth 인터페이스는 docker0 에 의해 호스트의 enp0s3(eth0) 과 연결된다.
			: 쉽게 생각하면 "실행중인" 컨테이너들 마다, (호스트 쪽에) 1개씩 붙어있는 인터페이스라고 봄 된다.

		1. docker0 >> (호스트의) eth0 과 (호스트의) vethXXXX를 이어주는 인터페이스.
			: 도커 자체 제공 Bridge 네트워크
			: 일반적인 interface 가 아닌, virtual ethernet "bridge"	
			: 외부와 통신할 경우 docker0 이 gateway 의 역할을 수행
				
				
			

	    : 컨테이너 (내부)
		1. eth0 : 컨테이너의 NIC. 컨테이너가 외부와 통신할 수 있기 해준다.


	: 호스트 <--> 컨테이너 간 파일 전송 >> docker container cp 명령어 활용
		: 실습에서는 터미널 2개 열고, 터미널 하나는 host 전용, 나머지 하나는 컨테이너 내부 전용으로 사용한다	
		: 어느 방향이건 해당 명령어는 호스트쪽에서 실행되어야함
			:호스트쪽에 docker 패키지가 설치되어있으니까
  





쿠버네티스 Kubernetes >> 컨테이너화된 애플리케이션의 자동 배포, 확장 및 관리를 해주는 오픈소스 opensource 플랫폼
	Kubernetes 어원 
		: Kubernetes 자체는 고대 그리스어로, 배의 조타수(Helmsman)를 의미
		: Kubernetes 를 K8s 라고도 부름
			: Kubernetes단어가 K 와 s 사이에 8 글자로 이뤄져서 그렇다

	Kubernetes의 역사
	    : 요약 >>
 		: Kubernetes는 2014 년 발표된 구글의 사내 프로젝트. 
			: Borg 의 영향을 많이 받음
			: Borg >> 수천~수백 만개의 작업을 실행/관리하는 구글 사내 시스템

		: 2016 년엔 Helm이 발표됨
			: Helm 헬름>> 쿠버네티스 패키지 관리 프로그램
				: 이후 chap9에서 자세히 다룸


	    : Kubernetes는 Container mangement system  중 하나
			: https://tech.osci.kr/kubernetes-%ED%83%84%EC%83%9D-%EB%B0%B0%EA%B2%BD-%EB%B9%84%ED%95%98%EC%9D%B8%EB%93%9C-%EC%8A%A4%ED%86%A0%EB%A6%AC/

			: Container mangement system >> 컨테이너 관리 시스템
				: 구글의 대규모 데이터 센터에서 효율적 작업을 위해 개발
				; 주요 오픈소스 Container mangement system 
					1. Borg >>컨테이너를 사용하여 prod ( 장기 실행 서비스 )와 batch(non-prod)작업을 효과적으로 관리하는 시스템
						: 구글 최초의 통합 컨테이너 관리 시스템
						: Linux 컨테이너 기술 기반

						:  클러스터 Cluster 와 셀 cell <<데이터 센터 장비들의 집합 단위
							Cluster >>  군집. 한 데이터센터내에서 서로 네트워크가 연결된 장비들.
								: 1개 이상의 셸cell 로 이루어짐

							셀 cell >> 여러개의 장비들을 하나로 묶은, Cluster 를 관리하기 위한 단위로, 보통 10000 대 정도의 장비를 의미
								: Cell 에서는 여러 Task가 돌아감	
								: 보통 수천명의 사용자가 하나의 borg 셀을 공유해서 사용



						: job과 Task 
							Task >> Borg 시스템에서 실행되어야 하는 개별 작업 단위. 컨테이너에서 실행되는 리눅스 프로세스들.
								: 분류
									(1) long-running 서비스(prod) >> 오래 돌아가는, "절대" 죽으면 안되는 서비스.
									(2) batch 작업(non-prod) >> 일시적으로 돌아가는 서비스



							Job>>  작업 그룹 . 여러 TASK를 그룹화한 단위
								priority >> 우선순위. 셀 내에서 실행/대기중인 JOB의 상대적 중요도.
									: 모든 Job 엔 priority 가 있다
									: 처리 가능한 양보다 많은 Task , JOB이 셸에 할당 될 경우 쓰임

								quota 쿼터>> Job 에서 사용될 수 있는 자원의 최대치 값
									: 어떤 잡이 스케줄링 가능한지 결정하는데 쓰임
									: CPU, RAM, 디스크 등으로 표현됨
									


						: alloc	 와 alloc set 
							 : 일반적으로 task 는 alloc 를 참조, job 은 alloc set 을 참조
							alloc >> 하나 이상의 TASK 들에 대한, 장비에 미리 할당해 둔 자원들 . 
							alloc set >> 여러 alloc들을 그룹화한 단위
 			
						:  BorgMaster >> Borg 시스템의 중앙 제어 역할. 
							: 사용자가 시스템과 상호 작용할 수 있는 API를 제공하고, 작업을 예약하고 스케줄링하며 클러스터 관리를 수행
							: 구성 >> 메인 borgmaster 프로세스 + 2개의 스케쥴러 프로세스
								: 스케쥴링하는데 꽤 오래 걸리기 떄문에 빠른 처리를 위해 borgmaster의 스케쥴러는 다른 기능들과 분리되어있음
								
								:  메인 borgmaster 프로세스 
									: 역할
										1. Borg 시스템내부의 모든 객체(장비, 태스크, alloc등)의 상태를 관리.
										2. Borglet 과 통신
										3. 컨트롤러(클라이언트 request 처리)

									: 논리적으로는 단일 프로세스지만 실제로는 5개의 사본을 가짐
										: 사본 >> 셸의 상태를 메모리에 복제해서 관리함
							: 하나의 borgmaster는 셀내의 수천개의 장비를 관리 가능

							: checkpoint >> Borgmaster의 특정 시점에서의 상태
								: paxos 저장소에 기간별 스냅샷으로 보관됨


						: 스케쥴러 scheduler >> Job의 자원조건 보고, 적당한 자원을 가진 장비에 Task를 할당
							: 작동 절차
								1. Job 이 제출되면 Borgmaster는 그걸 paxos 저장소에 저장, Job의 Task를 pending 큐에 추가
								2. scheduler가 pending 큐를 비동기적으로 확인해 적절한 장비에 Task 를 할당

							: 주로 Task 를 관리하는거지, Job을 관리하는게 아니다
							: 높은 우선순위를 가진 Task 가 먼저 스케쥴링 된다


						: Borglet >> 셀내의 모든 장비들에서 돌아가는 Borg 에이전트
							: 역할 
								1. borgmaster와 해당 장비가 통신 가능하게 함											:  borgmaster나 모니터링 시스템에 장비 상태 보고
		
								2. 태스크의 시작/정지. 실패했을때 재시작등을 담당
								3. 로컬자원 관리
							: borgmaster에 요청이 몰리는걸 막기 위해, borgmaster에서 몇 초 간격으로 borglet에서 장비의 상태를 요청한다
								: 이거 SMTP Simple Mail Transfer Protocol아님?


						: Borg 와 Kubernetes 
							: Borg 의 단점 
								1. Job은 Task 를 그룹화 하는 용도로만 사용 가능
									: Borg에선 멀티 Job을 하나로 처리하거나, 잡의 특정 하위집합만 묶어서 처리할 수 없어서 롤링 업데이트나 잡 크기변경등을 못한다.
									: Kubernetes는 label을 이용해서 pods들을 스케쥴링한다.  작업할 대상을 정한다.


								2. 장비당 하나의 IP 주소만 있어서 처리하기가 복잡
									: borg에서는 장비에 IP가 하나밖에 없어서 각 Task들이 장비의 포트를 구분해서 사용해야됨. 
										: 포트를 자원으로 간주하고 관리하는게 복잡했음 
											: 태스크가 얼마나 많은 포트를 사용할 지 등을 미리 정해놔야됨
									: Kubernetes는 모든 pod 와 서비스들이 각각의 IP 주소를 가짐. 포트를 관리할 필요가 없음


							: Borg 의 장점
								1. alloc은 유용
									: borg에서는 alloc을 이용해서 부가 서비스(웹서버가 사용하는 데이터를 주기적으로 업데이트하는 등)들을 별도의 팀에서 개발하는게 가능
									: kubernetes 에서는 alloc 와 유사한 개념으로 pod 가 있음(같은 pod내에 헬퍼 컨테이너를 사용가능)

								2. 클러스터 관리라는게 Task 관리만 하는건 아니다
									: borg에선 Task와 장비의 생명주기를 관리하는 것 뿐 아니라 네이밍, 로드밸런싱등의 서비스 제공
									: Kubernetes는 service를 이용해서 네이밍과 로드밸런싱을 지원
										: 네이밍 >> label selector를 통해서 pod에 동적으로 서비스 이름을 정의 가능. 그리고 정의된 클러스터내의 모든 컨테이너들은 서비스명을 기반으로 서비스에 연결 가능
										: 로드 밸런싱 >>  자동으로 서비스에 대한 커넥션들을 pod에 로드밸런싱

								3. Introspection이 중요하다
									: borg 에선 디버깅 정보를 모든 사용자가 볼 수 있다. 
									: Kubernetes는 borg의 introspection 테크닉을 많이 가져옴(
자원 모니터링을 위해 cAdvisor를 사용하고, Elasticsearch/Kibana/Fluentd 를 이용해 로그를 수집.  ) 마스터는 객체의 상태에 대한 스냅샷을 요청가능.  모든 구성요소들이 사용할 수 있는 이벤트(pod가 스케쥴되었는지, 컨테이너가 장애인지등)를 기록하는 표준화된 구조를 가지고 있고, 이걸 클라이언트가 이용할 수 있다.


								4. 마스터는 분산 시스템의 커널
									; Borgmaster는 처음에는 단일 프로세스로 시작했지만, 시간이 흐르면서 스케쥴러나 UI 그외 기능등을 별도의 프로세스로 분리해 내면서 스케일업하기에 좋은 구조로 발전

									: Kubernetes는 여기서 한발 더 나아가서 API 서버를 가짐 (클러스터 관리에 필요한 기능들을 마이크로 서비스로 만들어서 API 서버의 클라이언트가 되게 만들었다)


						: https://arisu1000.tistory.com/27785

					2. Omega >>  Borg 시스템을 개선시킨 시스템
						: 구글 두번째 컨테이너 관리 시스템. 
						: Omlet >> 애플리케이션을 컨테이너화하고 실행하는 작은 실행 단위.
							: 각 Omlet 은 개별적으로 구성되며, 컨테이너 클러스터 내에서 관리됨						: 클러스터에서 작업을 수행하는 데 사용되는 컨테이너. 

						: Cell >> 대규모 머신들을 관리하는 단위. 

						: CellState >> Omlets의 정보를 실시간으로 추적 
						: Cell Service >> Omlets이 필요로 하는 서비스 제공

					3. Kubernetes >> Omega, Borg 시스템을 개선한 시스템
						: 구글 세번째 컨테이너 관리 시스템. 
						: 구글의 최신 프로그래밍 언어인 Go를 기반으로 작성됨
						: 개발자가 클러스터에서 실행되는 복잡한 분산 시스템을 쉽게 배포하고 관리하기 위해 설계됨

	
	Kubernetes란? 역할은?
		: 수 많은 컨테이너를 쉽게 관리하게 해주는 시스템.  복잡한 애플리케이션의 운영을 자동화가능
			: 걍 생각해봐도 100 개의 컨테이너가 있을 때, docker container run 을 100 번 치는건 좀.. 음.. 개노답

		: 서버를 다수 운영하는 상황에서, 서로 다른 서버에서 작동하는 컨테이너를 한꺼번에 쉽게 관리 가능
		: 클라우드 환경에서 유용하게 사용됨
 		: 계속해서 Desired State (원하는 상태)를 만들기 위해 Current State(현재 상태)를 바꾸는 플랫폼
			: 그러니까 컨테이너/네트워크 .. 등에 대한 desired state 를 쿠버네티스에게 알려주면 쿠버네티스는 계속해서 current state를 체크한다
 

	Kubernetes의 구조
		: 도커에 비해 좀 더 구성이 복잡
		: 구성 요소<<  교재 +구글링 통합 정리 
			: https://ooeunz.tistory.com/118
			: https://velog.io/@youknowwhat/Kubernetes-%EA%B8%B0%EB%B3%B8-%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8
			: 워크 로드 workload >> 쿠버네티스에서 실행되는 애플리케이션
				:  쿠버네티스는 pod 내부에서 workload를 실행하게 됨


			: Cluster, Node , Pod, Service 
			    : https://eng-sohee.tistory.com/129

			    : 파드 Pod >> 하나 이상의 컨테이너 집합.
				: 쿠버네티스 상의 단위 중 가장 작은 단위
				: Pod 는 Object 로, 이에 대해 자세히 아래의 Object 부분에 기술함.


			    : 서비스 Service >> 하나 이상의 pod 집합.
				: 이에 대해 자세히 아래의 Service 부분에 기술함.


			    : 노드 node >> 클러스터를 이루는 컴퓨터(물리적 컴퓨터 혹은 가상머신) 단위
				: 클러스터를 구성하는 기본 단위이고, 클러스터 다음으로 큰 단위임.
				: 각 node는 고유한 IP 주소를 가진다 
				: node의 주요 컴포넌트
					: 그러니까 Worker Node 이든, Master Node 이든 Node 라면 다 아래의 컴포넌트를 가진다는 말이다
						: 근데 Master Node는 특성상 Control Plane 을 통한 클러스터 관리에 초점이 맞춰있지, Pod 실행 및 관리에 초점이 맞춰져 있는게 아니라 (Master Node에도 Kubelet, Container Runtime, kube-proxy 이 존재하긴 하지만) Kubelet, Container Runtime, kube-proxy 의 사용 빈도가 ( Worker Node 에 비해 ) 적은 편이다.

					1. Kubelet >> 각 노드에서 실행되는 Agent로, 해당 노드 를 구성하는 Pod 들의 관리를 담당
						: 역할
							1. 해당 노드에 배포된 모든 Pod 가 정상적으로 동작하도록 관리
								: pod 의 상태를 모니터링 하고, pod 상태에 이상이 있을 시 해당 pod 를 다시 배포

							2. MasterNode 의 API 서버와 통신하여, 노드 자체의 상태와 노드 내에서 실행중인 파드들의 상태를 지속적으로 보고
								: MasterNode가 노드의 상태를 모니터링할 수 있게 해준다

						: Kubelet 이 Pod 를 관리하는건 맞는데, 실제로 Pod 내의 컨테이너들을 직접적으로 관리하는건 아니고 해당 노드의 Container Runtime 에게 명령을 내림으로써 관리한다


					2. Container Runtime 컨테이너 런타임>> 해당 노드를 구성하는 Pod 들 내의 컨테이너들을 (실제로. 직접적으로.) 관리하는 소프트웨어

						: Kubelet의 지시에 따라 컨테이너를 시작/중지/관리하며, Pod 내에서 컨테이너가 올바르게 실행되도록 한다			
							:  Container runtime interface 컨테이너 런타임 인터페이스>> Kublet 과 Container runtime 간의 통신을 가능하게 함
								: Kubelet 과 API server 의 통신은  kube-proxy의 관여 없이, 직접적인 HTTP/HTTPS 기반의 REST API 호출로 이뤄진다.

						: Kubelet 처럼 Container Runtime 도 노드 당 하나씩 만 존재하고, 하나의 Container Runtime 으로 해당 노드의 모든 파드, 컨테이너의 라이프 사이클을 관리하는거다

						: 대표적인 container runtime >> containerd, CRI-O
							: 실습에서는 가장 보편적인 containerd 를 사용


					3. kube-proxy>>  중개자로써 , "외부에서 서비스까지의 트래픽 라우팅" 과 "서비스에서 특정 pod 로의 트래픽 라우팅"을 관리
						: Kubelet, Container Runtime 과 마찬가지로 각 노드 당 하나씩 존재
						: 기능
							1. 트래픽 라우팅 및 로드 밸런싱
								: service IP 를 통해 들어오는 트래픽을 해당 service 에 전달해준다
								: serivce 로 들어온 트래픽을 특정 Pod 로 전달해준다
									: 하나의 서비스에 여러 pod 가 연결된 경우, 로드밸런싱 해준다
		
							2.  가상 IP 관리
								: Service 의 고유한 고정 IP인 ClusterIP를 관리. 해당 IP를 실제 Pod IP 로 매핑해준다.

							3. 네트워크 규칙 설정
								: 어떻게 라우팅 시킬지 규칙 설정 가능

						: 다른 Node 에 속한 Pod 와 해당 Node 의 Pod 가 통신할 떄는 pod-proxy 가 관여하지만, 같은 Node에 속한 Pod 끼리의 통신은 kube-proxy 의 관여 없이 로컬 네트워크를 통해(서로의 IP주소를 사용하여) 이뤄진다.




					4. 노드의 상태 >> 해당 Node 가 클러스터 내에서 정상적으로 동작하는지의 여부를 나타냄
						:  Ready, NotReady, Unknown 등 이 있음



				: 종류 
					1. Master Node 마스터 노드 >> 클러스터 전체를 관리하는 노드
					    : Master Node가 죽으면 클러스터를 관리할 노드가 없기에, 일반적으로 3개 정도의 마스터 노드를 띄우는 편
					    : Master Node 에서는 일반적으론 Pod 를 실행하지 않는다
						: Master Node 에서도 Pod 를 실행할 수 있으나, Master Node 는 클러스터 관리가 주 목적이라 Control Plane 의 운영에 초점을 맞추기 위함.

					    : Master Node 의 컴포넌트(구성 요소) << Control Plane 이 다는 아니다. 이외의 자잘한 구성요소들도 있다.. 
						(1) Control Plane 컨트롤 플레인 >> 쿠버네티스 클러스터 전반의 작업을 관리하는 , 쿠버네티스 클러스터의 두뇌 같은 부분
							: Control Plane 의 구성 요소 
								:  Control Plane 은 Master Node 의 일부 컴포넌트 집합을 지칭하는 거라고 봄 된다.
								1.  kube-apiserver >>   API 서버. control plane에서의 프론트앤드 역할을 함
									: 그러니까 클라이언트(사용자, 애플리케이션, 컴포넌트 등)로부터의 request을 받아들이고, 이를 적절한 컴포넌트에게 전달해준다
										: 브라우저와 같은 클라이언트 뿐 아니라 클러스터의 컴포넌트들 간의 정보 교환도 얘가 중재한다는 것.

										: 즉, API 서버는 쿠버네티스의 중앙 통신 허브
											: 쿠버네티스에서 (kubectl 명령어와 같은) 모든 인터랙션이 이 API 서버를 통해 처리된다
												: ex) 컨트롤러는 API 서버에 주기적으로 request을 보내어, 자신이 관리하는 리소스의 현재 상태를 확인

									: 여러 인스턴스를 한번에 실행 가능
									: 부하 분산을 통한 트래픽 관리 가능


								2.  ectd 엣시디 >> 클러스터의 모든 상태 데이터를 key-value 형태로 저장하는 데이터베이스 역할
									: 이를 통해 클러스터의 상태를 특정 시점으로 되돌릴 수 있다
									: etcd 는  etc 디렉터리(/etc) 와 distributed (터뜨리다) 의 합성어
									: 보통 etcd는 클러스터링 되어있다				
										: etcd 클러스터링 에서의 노드 >> etcd 프로세스가 실행되는 "인스턴스"
											: 인스턴스 >> 서버 또는 가상 머신
												: API server 와는 별개인, Master Node 를 구성하는 또다른 서버를 의미.
													: (Master Node 의 일부 컴포넌트 집합을 Control plane 이라고 하는거지) Master Node 가 Control Plane 으로만 이뤄진게 아님을 기억하자. (Control Plane 에 서버는 API 서버밖에 없지만, 그렇다고 Master Node 를 구성하는 서버가 API 서버밖에 없다는 게 아니라는 것)
											: 노드라고 worker node 를 의미하는거 아님 주의

										: 그러니까 etcd 가 클러스팅 되어있다 == etcd라는 분산 키-값 저장소는 여러 개의 서버에 걸쳐 분산되어 실행된다
										: etcd 클러스터는 최소 3개의 노드로 구성되는 것이 일반적
											: 5개 또는 그 이상의 노드로 구성할 수도 있음
											: 주로 홀수 개로 하는데, 이 이유 >> quorum 알고리즘을 사용하여 데이터의 일관성을 보장하기 떄문
  
										: 왜 서버 하나에서 etcd 를 돌리는게 아니라 여러개로 분산해서 실행시킬까>> 클러스터의 장애 복구를 위해 다중 인스턴스로, 데이터의 복제를 통해 고가용성을 유지하기 위함




								3. kube-controller-manager >> 컨트롤러 매니저 . 여러 controller들을 관리/실행하는 역할. 
									: 컨트롤러 매니저는 여러 controller 들을 단일 프로세스로 관리
										: 즉, 여러 컨트롤러들 각각은 논리적으로 개별 프로세스지만(=각각 다른 리소스를 관리하니까) , 모두 kube-controller-manager라는 하나의 프로세스에서 실행된다
										: 여러 컨트롤러 작업들이, 하나의 프로세스에서 동시다발적으로 작동한다

										: 왜 단일 프로세스로 관리? >> 그게 더 효율적이니까...
											: 각 컨트롤러를 독립된 프로세스로 실행하는 것보다 단일 프로세스로 여러 컨트롤러를 실행하는게 리소스를 더 효율적으로 사용가능
											: 하나의 프로세스만 모니터링/관리하면되니까 관리 및 유지보수가 더 쉬움
											: 단일 프로레스로 관리하면 컨트롤러 간 통신이 더 원활


									: 그러니까 controller 는 리소스를 컨트롤하고, contoller manager 는 이러한 controller 를 컨트롤한다는 것



								4. kube-scheduler >> 스케쥴러. 새롭게 생성된 Pod를 적절한 Node에 할당하는 자동화된 스케줄링 프로세스를 담당.
									: 클러스터의 효율적인 리소스 사용과 파드의 성능, 가용성을 유지를 가능하게 함


									: 스케줄링 >> Pod 를 특정 Node 에 배치하는 것	
										: Pod 가 생성될 때마다 자동으로 수행됨
										: 과정
											(1) 적합한 노드 집합 필터링 >> (파드가 요구하는 특정 리소스( CPU, 메모리.. )나 노드에 설정된 제약 조건등을 고려해) 모든 노드 중에서 파드를 실행할 수 있는 적합한 노드 집합을 필터링
											(2) 점수 매기기 Scoring >> (구한 노드 집합에서) 각 노드에 ( 노드의 리소스 사용률, 파드 간의 친화성 및 반감성 규칙, 네트워크 토폴로지 등을 기준으로) 점수를 매김	
											(3) 최적의 노드 선택 >> 점수가 가장 높은 노드를 선택해 해당 노드에 파드를 배치.

										: 여러가지 알고리즘(전략)이 사용됨
											: "최소화된 리소스 사용", "균등한 로드 분산", "고가용성 유지" 등
									: Affinity 와 Anti-affinity 
										: 친화성 Affinity >>  Pod를 서로 가까이 배치하는  규칙
											:  특정 노드에 파드를 선호하게 함

										: 반감성 Anti-affinity >> Pod를 서로 멀리 배치하는  규칙
											: 특정 노드나 동일한 노드 그룹에 파드가 배치되지 않도록 함

									: 사용자 정의 우선순위와 선호도 규칙을 사용하여 특정 Node가 Pod 를 더/덜 선호하게 할 수 있다 
```

					2. Worker Node >>   배포하고자 하는 어플리케이션의 실행을 실제로 수행하는 노드
						: 이 노드에 애플리케이션이 올라간다



			    : 클러스터 Cluster >> 같은 목적으로 묶인, 여러 대의 컴퓨터 집합
				: 쿠버네티스 상의 단위 중 가장 큰 단위
				: 마스터노드 + 워커노드 한 것.
				: (node와는 달리) 클러스터엔 IP가 할당되진 않는다.
					:  헷갈리지 말야할 것은, 이후 나오는 ClusterIP 는, 클러스터에 대응되는 IP 가 아니라 클러스터 내부에서만 사용가능한 ip 이다





			: Label 과 Selector 
				: Label 레이블 >>   Object를 구분/관리하기 쉬우라고 사용자가 할당하는 특정값
					: 꼬리표 같은 느낌
					: Key-Value 형식
						: 그러니까 yaml 파일에서 "어쩌구항목 :저쩌구값 " 으로 기술된다는 거임
						: 사용가능한 key명이 따로 정해져있는건 아니고 (해당 리소스 내에서 고유하다면) 맘대로 정의 가능

					: 모든 Object 에는 label 을 태그 가능(=달 수 있다) 
						: 주로 pod 에 태그됨

					: label 은 수정이 가능하다
					: how to use
						: label 정의 >> pod 를 생성할 때, metadata 아래의 labels 부분에,  이 pod 에 대한 고유한 라벨 정의 
						   : 그러니까 아래와 같은 형식으로 정의
							apiVersion: v1
							kind: Pod

							metadata:
							  name: Pod이름
							  labels:
							    키명1: 어떤값
							    키명2: 어떤값
								.
								.
								.

						: label 활용 >> service 를 생성할 떄, spec아래의 selector 부분에 특정 label 을 입력하여 생성
						    : selector를 통해 key:value로 해당 내용과 매칭되는 레이블이 있는 파드와 연결되는거다

						    : 그러니까 아래와 같은 형식으로 정의
							apiVersion: v1
							kind: Service

							metadata:
								.
								.
								.

							spec :
							  selector: 
							    키명1: 어떤값
								.
								.
								.


						: pod 에 부착된 label 목록 확인 >>  
							(1) kubectl get pod --show-labels >> 모든 label 에 대한 pod 목록 확인
							(1) kubectl get pod  -L 키1,키2,키3..>> 특정 label들에 대한 pod 목록 확인




				: Label Selector >> Label 을 이용해 특정 리소스를 선택(필터링)하여 원하는 작업을 수행
					: selector 의 선택 기준
						

			: Resource 와 Object
			******:  리소스와 오브젝트는 실제론 다른 개념이긴 하지만, 쿠버네티스 문맥에서 리소스란 표현은 오브젝트로 혼용되어 많이 쓰인다.
					: 리소스는  추상적인 스펙정의 , 오브젝트는 그러한 리소스의 구체적인/실제하는 인스턴스로 개념이 다르지만 쿠버네티스에서 등장하는 "리소스"란 표현에 "오브젝트"를 넣어 해석해도 괜찮다는 말이다.(대부분의 경우에)

				: Resource 리소스>> 쿠버네티스에서 관리하는 "추상적인" 스펙 정의(엔터티). Object 를 생성하기 위한 템플릿.
					: 클러스터에서 관리될 수 있는 다양한 엔티티의 유형을 정의한 것.
					: 쿠버네티스에서는 「리소스」를 등록하는 것으로 컨테이너의 실행과 로드밸런서의 작성을 비동기로 실행

					:  리소스의 정의 >> YAML 또는 JSON 파일로


					: 종류 	
					*******(1) Workloads 리소스 >> 컨테이너의 실행에 관한 리소스
							1. Pod >> 컨테이너 그룹을 실행하는 리소스
								: 가장 기본적인 실행 단위
							2. Replicaset 레플리카셋 >>특정 갯수로 pod의 복제본을 유지
								: 클라이언트가 요구하는 복제본 개수만큼 pod 를 복제 및 모니터링

							3. Deployment 디플로이먼트 >> 배치. 애플리케이션의 배포와 스케일링을 관리
							4. DaemonSet 데몬 셋 >> 쿠버네티스의 모든 노드가 pod의 복사본을 실행하게 함
								: 쿠버네티스 클러스터에 새로운 노드가 추가되면 pod 역시 추가됨 
								: 주로 (로깅, 모니터링, 스토리지 와 같은) 시스템 수준의 서비스를 실행하는데 사용됨
							5. StatefulSet 스테이트풀셋 >> pod 사이에서 순서와 고유성이 보장되어야하는 경우 사용
							6.  Job 과 Cronjob >> Task 가 정상적으로 완료/종료되는 것을 담당
								: pod 가 정상적으로 종료되지 않는다면 재실행시킴
								: Job >> 작업이 한 번 종료 되는 것을 담당
								: Cronjob >> 리눅스의 Crontab 과 비슷한 역할




				



						(2) Discovery & Load Balance 리소스 >> 컨테이너를 외부에 노출하기 위한 엔드 포인트를 제공하는 리소스. 네트워크 관련 리소스.

							1. Service 서비스 >> 클러스터 내에서 실행 중인 Pod 집합에 대한 네트워크 접근을 제공하는 리소스
								: 기능
									0. 외부 접근 >> 클러스터 외부에서 내부 파드에 접근할 수 있도록 함
										 :Service를 통해 실행중인 pod 의 수정 없이도 외부에 노출 시켜 클라이언트와의 통신이 가능해진다

									1. 서비스 디스커버리 >> 클러스터 내에서 Pod 간의 통신을 용이하게 함
										:   파드의 IP 주소 대신 하나의 고정된 IP 주소와 DNS 이름을 제공.
											: 파드가 생성되거나 삭제되어 IP 주소가 변경되더라도, Service를 통해 안정적인 네트워크 연결을 유지할 수 있다.


									2. 로드 밸런싱>> 다수의 Pod 에 대한 트래픽을 자동으로 분산시켜 과도한 부하 방지
										: 쿠버네티스는 이를 위해 ClusterIP와 같은 내부 IP를 할당하고, 해당 IP를 통해 트래픽을 관리



								: 타입
									(1) ClusterIP : 클러스터 내부에서만 접근 가능한 IP 주소를 제공
									(2) NodePort : 클러스터의 각 노드에서 고정된 포트를 열어 외부에서의 접근을 허용
									(3) LoadBalancer : 클라우드 환경에서 외부 로드 밸런서를 생성하여, 외부에서 직접 서비스에 접근할 수 있게 함
									(4) Headless (None)
									(5) ExternalName : DNS 이름을 외부 서비스로 매핑하여 클러스터 내에서 외부 서비스에 접근할 수 있게 함


							2. Ingress 인그레스 >> 클러스터 외부에서 내부 서비스로의 HTTP(S) 트래픽을 제어하고 관리하는 리소스	
								: Service 보다 더 높은 수준의 네트워크 리소스 
									: Service 리소스를 통해 트래픽을 전달
										: Service 리소스를 대상으로 동작하며, 여러 Service로의 트래픽을 라우팅
									:  Service 보다 더 높은 수준 == Ingress가 Service보다 더 세부적이고 복잡한 네트워크 트래픽 관리 기능
										: Service 는 기본이되는 네트워크 리소스
										: Ingress는 Service 위에서 동작하는 좀 더 세부적인, 정교한 네트워크 리소스


								: 기능
									0. HTTP(S) 라우팅 >> 클러스터 외부에서 내부 파드에 접근할 수 있도록 함
										: URL 경로나 호스트 이름 기반의 외부에서 클러스터로 들어오는 HTTP(s) 트래픽을, 적절한 서비스로 라우팅 시킨다
										: 단일 외부 IP 주소를 사용해 여러 서비스에 대한 요청을 관리 가능

									1. TLS/SSL 제공 >> 보안 강화

									2. 리버스 프록시 역할 >> 사용자가 지정한 규칙에 따라 트래픽을 각 서비스로 전달 가능
										: Ingress는 여러 서비스에 대한 접근을 중앙 집중적으로 관리

								: Ingress 컨트롤러 
									: Ingress 리소스는 직접적으로 트래픽을 관리하지 않음. Ingress 컨트롤러에 의해 트래픽의 관리가 수행됨
									: Ingress 리소스에 정의된 규칙에 따라 Ingress 컨트롤러가 트래픽을 적절하게 라우팅한다.




						(3) Config & Storage 리소스 >> 설정/기밀정보/Persistent Volume 등에 관한 리소스

							1. Secret
							2. ConfigMap
							3. PersistentVolumeClaim


						(4) Cluster 리소스 >> 클러스터 자체의 행동을 정의하는 리소스
							: 보안 설정과 정책, 클러스터의 관리성을 높임
							1. Node
							2. Namespace
							3. PersistentVolume
							4. ResourceQuota
							5. ServiceAccount
							6. Role
							7. ClusterRole
							8. RoleBinding
							9. ClusterRoleBinding
							10. NetworkPolicy


						(5) Metadata 리소스 >>	클러스터 내의 다른 리소스를 조작하기 위한 리소스
							1. LimitRange
							2. HorizontalPodAutoScalar
							3. PodDisruptionBudget
							4. CustomResourceDefinition


				: https://somaz.tistory.com/198
				: https://thecodingmachine.tistory.com/9


				: Object >> 쿠버네티스 클러스터의 상태 관리를 위한 객체.  resouce를 기반으로 생성된 "실제" 인스턴스
					: https://velog.io/@youknowwhat/Kubernetes-%EA%B8%B0%EB%B3%B8-%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8


					: 즉 쿠버네티스 시스템의 도메인을 모델링 한 것 
					: 영속성을 가진다
						: 영속성 persistence >> 프로그램이 종료되도, 사라지지 않는 데이터 특성
					: 생성 의도를 가지고 있다
						: Desired State가 기술된 Object를 생성하면 쿠버네티스 시스템이 해당 Object의 Desired State를 보장하는 방식으로 동작한다

					: Object 정의 >> yaml 이나 json 같은 파일로  (요즘엔 거의 yaml)

					: 종류
						: Object 가 Resource 를 기반으로 만들어진거긴 해도 Workloads, Discovery & Load Balance  , Config & Storage, Cluster,  Metadata 같이 리소스의 분류기준으로 안나누고 Basic Object 와 Basic Object 가 아닌 Object들로 분류한다.

						(1)  Basic Object 기본 오브젝트>> 쿠버네티스에서의 가장 기본적인 Object
							1. Pod >>  쿠버네티스에서 생성/관리 가능한 작은 컴퓨팅 단위

								: 독립적인 공간과 IP를 갖는다. 
								    : IP 
									: 파드가 생성될 때 고유의 IP 주소가 할당됨
									: 가변적이다
										: 파드 삭제 및 재생성 시, IP 주소는 바뀐다
										: 이로 인한 문제 해결 방안으로 Service 라는 Object 가 존재한다
									: 쿠버네티스 클러스터 내에서만 해당 IP를 통해 해당 파드로 접근이 가능하고 외부에서는 해당 IP로 접근할 수 없다.

									
								:  쿠버네티스에서는 컨테이너가 pod 내에서 실행된다
									: 컨테이너 
										: 하나의 독립적인 서비스/기능을 구동 가능 
	
										: 서비스(컨테이너)끼리 연결/통신될 수 있도록 포트를 가진다
											: 한 컨테이너가 포트를 하나 이상 가질 수 있지만, 동일한 파드에 속한 컨테이너끼리 포트가 중복될 순 없다
											: 여러 기능을 묶어 하나의 목적으로 배포해서 사용 가능

									: 하나의 파드는 1개 이상의 컨테이너를 가진다
									: 그러니까 pod == 컨테이너를 그룹화 한 것
										: 도커에서는 최소 실행단위가 컨테이너 였다면(도커에서는 컨테이너가 단독으로 실행됬었다), 쿠버네티스의 최소 실행 단위는 pod 라고 봄 된다
									: 하나의 pod 에 속하는 컨테이너들은 하나의 목적을 위해 구성된 컨테이너들.


									: 컨테이너들을 파드라는 하나의 호스트로 묶어 관리한다고 보면된다
										: 그래서 localhost를 이용해 한 컨테이너에서 다른 컨테이너로의 접근이 가능하다



								: 다수의 pod 들은 여러 worker node 에 분산되어 실행됨
									: 그러니까 서로 다른 pod 들은 서로 다른 노드에서 실행될 수 있지만, 하나의 pod 가 분할 되어 여러 노드에 실행되는 일은 없다 ( 쿠버네티스의 최소 실행단위는 pod)






							2. Service >> Pod 접속을 안정적으로 유지하기 위한 기능을 제공하는 오브젝트
								: Pod 는 언제든 죽거나 재생성될 수 있는 유동적인 오브젝트임에도 안정적인/고정적인 접속을 가능하게 해준다
								: Pod 와는 달리 사용자가 직접 지우지 않는 한 삭제/재생성되지 않으므로 고정된 접속 정보를 가져 안정적
								: 각 Pod의 IP로는 외부에서 직접적인 접근이 불가능하지만, 서비스는 이를 가능하게 해준다.
								: 하나의 service는 여러개의 Pod와 연결될 수 있다
									: 여러개의 Pod 와 연결되는 경우 Serive 가 트래픽을 분산하여 Pod 에게 전달해준다. 
										: 그러니까 (service 오브젝트의 spec 정의 부분의) selector 를 통한 로드밸런싱이 가능

								: 타입 
								    : yaml 파일의 type 부분에서 설정 가능
									(1) ClusterIP : 클러스터 내부에서만 사용 가능한 IP 주소를 제공
										: Service 의 디폴트 타입
										: 외부에서는 ClusterIP를 사용한 접근이 불가하지만, 클러스터 내부에서는 "ClusterIP:서비스포트" 의 형식으로 접근이 가능

									(2) NodePort :  클러스터 외부로부터 클러스터 내부의 Pod 으로의 트래픽을 받기 위해 사용하는 서비스 유형 


 										: https://yoonchang.tistory.com/49
										: NAT 컨셉.	
											: 그러니까 처음에 "어떤IP:포트A" 로 요청된 트래픽을 "저런IP:포트B"로 ip를 다르게하여 전달시켜줄 수 있다는 것.

										: 작동 방식 >> 클러스터의 각 노드의 고정된 포트를 열어 외부에서의 접근을 허용함
											1. NodePort 서비스에 대해 YAML 파일에 정의함 .

											    : 아래와 같이 정의된다
 
												type: NodePort
												ports:
												  - nodePort: NodePort서비스가사용할포트번호
												    port: 라우팅당할서비스의포트번호
												    targetPort: 라우팅당할서비스내의Pod내의특정컨테이너의포트번호


												: nodePort >> nodePort 서비스가 어떤 포트를 listen 할것인지 설정 
													: 30000~32767 범위 내에서 할당 가능. (지정하지 않으면 해당 범위에서 자동으로 할당됨) 

												: port>> nodePort 서비스로 받은 트래픽을, (해당 노드 내의) 어떤 포트를 listen하는 서비스에게 전달할 것인지를 설정

												: targetPort >> nodePort 서비스에게 트래픽을 전달받은 해당 노드 내의 서비스가, 어떤 포트를 가지는 컨테이너에게 최종 전달할 것인지를 설정



											2. 클라이언트가 "어떤노드의IP:nodePort" 라는 request 보냄
												: 어떤노드의IP 라는건, 클러스터 내에 정의된 노드들의 IP 중 아무거나 하나란 의미. 
													: 그러니까 해당 클러스터 내의 모든 노드들은 모두 자동적으로 nodePort 에 대응하는 포트를 listen 하고 있는 꼴이 되는 것.


											3. (nodePort 가 쓰는 포트로 request 가 보내졌음으로) NodePort 서비스가 해당 트래픽을 받음

											4. NodePort 서비스가 자신이 받은 트래픽을, ( 트래픽의 "어떤노드의IP:nodePort" 부분에서 ) 어떤노드의IP 에 대응하는 IP를 가지는 node내 의 서비스 중 특정 포트값(해당 NodePort 서비스의 port 부분에 정의했던 값)을 가지는 서비스에게 라우팅. 


											5. 해당 서비스에 연결된 Pod 들 중 특정 포트값(해당 NodePort 서비스의 targetPort 부분에 정의했던 값)을 가지는 컨테이너를 가지는 Pod 에게 라우팅. 

										: NodePort서비스의 externalTrafficPolicy 설정 >> 외부 트래픽을 어떻게 제어할지에 대한 설정
										    : 할당 가능한 옵션. 모드.
											(1) Cluster >> 트래픽을 해당 노드뿐만 아니라, 클러스터 내의 다른 노드에 있는 파드로도 전달할 수 있게 설정
												: 디폴트

											(2) Local >> 트래픽이 들어온 노드 내의 파드로만 트래픽을 전달 가능하도록 설정


										: NodePort 타입으로 만들어도 기본적으론 ClusterIP가 할당된다	
											:  그러니까 Nodeport 타입은 ClusterIP 타입의 기능들을 포함하고 있다. 




									(3) LoadBalancer : 클라우드 환경에서 외부 로드 밸런서를 생성하여, 외부에서 직접 서비스에 접근할 수 있게 함
									(4) Headless (None)
									(5) ExternalName : DNS 이름을 외부 서비스로 매핑하여 클러스터 내에서 외부 서비스에 접근할 수 있게 함

							3. Volume >> 컨테이너가 종료되더라도 파일 시스템을 유지시키고, 파드 내 다른 컨테이너들과 파일을 공유하는 것을 가능하게 하는 오브젝트
								: 쿠버네티스의 컨테이너들은 각각 자체적으로 파일시스템을 가지긴하지만, 컨테이너 내부에 저장되는 파일들은 수명이 짧다 
									: 컨테이너가 삭제/재실행되면 해당 컨테이너 내부의 파일들은 모두 삭제되니까

								: volume 의 종류
									(1) emptyDir >> Pod가 node에서 실행되면 자동으로 생성되는 임시 디렉토리
										: Pod 내의 컨테이너 간 "임시데이터"를 저장/공유하기 위한 볼륨
											: Pod 가 삭제되면 함께 삭제되기 때문에 주로 임시 데이터 저장용으로 쓰인다.

										: Pod 의 생성/삭제 와 함께 emptyDir도 함께 생성/삭제됨

										: 이 volume 의 이름이 emptyDir 인건, 처음 생성될 때 volume 이 비어있기 때문이라함.



									(2)  hostPath >> node 의 파일 시스템의 (특정 path 에 대응하는) 특정 디렉토리를 Pod 에 마운트

										: Pod 와 독립적으로 존재
											:  node 의 파일시스템에 저장된 데이터를 각 Pod 들이 마운트해서 가져오는거라, Pod 들이 삭제되어도 (해당 node 의 파일시스템의) 원본 데이터는 삭제되지 않는다

										: 각 노드에는 해당 노드만을 위해 사용되는 시스템 파일이나 설정 파일 등이 있는데, 이러한 파일들을 Pod 가 이용할 때 주로 hostPath를 사용한다.

										:  hostPath 로 마운트당할 path 는, (mount 이용할) pod 의 생성 이전에 노드에 미리 만들어져있어야된다. (안그럼 에러 난다)

									(3) PVC/PV >> 
										: https://kimjingo.tistory.com/153
										:  PVC/PV는 User와 Admin으로 역할(관점)을 구분해서 볼륨을 운영하게 됨
										    : 주의 >> PV 를 Admin , PVC 를 User 단이라고 부른다는게 아니라, Admin 이 pv 를 정의/관리할 수 있고 User 가 PVC 를 정의/관리 가능하단 말임.
											: Admin 관리자>> PV 를 정의
												:  Admin은 ( User들이 활용하게 될) 쿠버네티스 클러스터 자체를 설치/설정/관리하는 권한을 가진 사람

											: User 사용자 >> PVC 를 정의
												: User 는 end user(애플리케이션을 사용하게되는 클라이언트) 를 의미하는게 아니라, ( Admin이 제공한 ) 쿠버네티스 환경에서 실행되는 애플리케이션의 개발자/운영자를 의미하는거임

										: PVC/PV 의 생명 주기 >> 
											1. Provisioning 프로비저닝 >> PV를 만드는 단계
												(1) 정적(static) 프로비저닝 >> 클러스터 관리자가 미리 PV를 만드는 것 
												(2) 동적(dynamic) 프로비저닝 >> user 의 (PVC를 통한) 요청에 따라 PV를 만드는 것
													: User가 PVC를 생성했는데 해당 PVC의 요청에 적합한 PV가 없다면 쿠버네티스가 자동으로, dynamic 하게 , PV를 생성한다.


											2. Binding 바인딩 >>  프로비저닝으로 만든 PV를 PVC와 연결하는 단계
												: (PVC 생성 시) PVC에 정의된 요청에 따라 (쿠버네티스가) 적절한 PV를 자동으로 할당해줌

												: PV와 PVC의 바인딩은 디폴트론 1대1관계
													: 이는 스토리지가 다른 PVC에 의해 덮어쓰이거나 잘못된 접근이 발생하지 않도록 보장해준다.
													: 디폴트값이 1대1 바인딩이긴 해도, ReadWriteMany (RWX) 접근 모드를 사용하면 하나의 PV를 여러 PVC에 바인딩하여 하나의 스토리지를 공유하게 할 수 있다
														: 물론 여러 PVC에 의해 바인딩당하는 PV의 유형이 NFS, GlusterFS, CephFS와 같은 RWX 모드를 지원하는 유형이여야된다.
														: PV/PVC는 단일 Pod 의 스토리지 용 뿐 아니라, 여러 Pod 들 간의 공유 스토리지용으로도 많이 쓰인다.

												: 바인딩이 완료된 PV는 다른 PVC가 사용할 수 없으며, 해당 PVC가 삭제되거나 해제되기 전까지는 바인딩된 상태로 유지됨
													: PVC는 해당 PVC를 사용하는 pod 가 삭제되기 전까진 자동으로 삭제 안된다. 
													: 이러한 기능을 'Storage Object in Use Protection' (사용 중인 스토리지 오브젝트 보호)라고 함


											3. Using 사용 >> Pod 가 PVC를 볼륨으로 인식해서 사용


											4. Reclaiming 반환 >> 사용이 끝난 PVC가 삭제되고, PVC를 사용하던 PV를 초기화(reclaim)하는 과정
												: 초기화 정책에는 Retain, Delete, Recycle이 있다



										: PV PersistentVolume >> 클러스터 내의 영구적인 스토리지.  볼륨 자체. 

											: 클러스터 단위로 정의/생성됨 
											: 여러 스토리지 타입(NFS, AWS EBS, , Azure Disk ...등) 으로 정의 가능

											: pod 와 독립적으로 존재
												: pod 가 삭제되어도, PV 의 데이터가 삭제되진 않는다


											: 정책 정의 가능
												: 스토리지로의 접근 모드 등을 설정 가능




										: PVC PersistentVolumeClaim >>  Pod 가 필요로 하는 스토리지를 요청하기 위한 오브젝트.
											:  PVC를 굳이 도입한 이유 >> 중간에 PVC를 두어 Pod 와 스토리지를 분리함으로써 각각의 상황에 맞게 다양한 스토리지를 사용할 수 있다
												: (AWS EBS, NFS 등 .. ) 다양한 스토리지를 PV로 사용할 수 있는데, PVC를 사용함으로써 Pod 는 어떤 스토리지를 사용하는지 신경쓸 필요가 없어진다
												: PVC는 스프링/스프링부트의 JDBC처럼 스토리지(PV)의 유형에 상관없이 사용할 수 있게 해준다


											: User 에 의해 Pod 단위로 정의/생성됨 
										
											: PV 와 마찬가지로 PVC 도 YAML 로 정의된다
	


							4. Namespace >> 단일 클러스터에서 리소스(오브젝트)를 논리적으로 분리/그룹핑하여 관리할 수 있게 해주는 오브젝트
								: https://gwonbookcase.tistory.com/28

								: (Pod 로 한정된 게 아닌) "리소스"를 논리적으로 분리하는 것이라는 표현에서 알 수 있다 싶이, pod 뿐 아니라 service 와 같은 것도 같이 그룹핑할 수 있다는 말이다
								: Namespace로는 Cluster 리소스(Node, PV, Namespace, StorageClass , ClusterRole ,  ClusterRoleBinding )가 아닌 리소스( PVC, Pod, 여러유형의 Service , Deployment .. )들에 한하여 그룹핑 가능하다 
									: 모든 유형의 리소스를 Namespace 로 그룹핑할 수 있는건 아니란 말이다.
									: 왜 Cluseter 유형의 리소스(오브젝트) 들만 그룹핑이 안되는걸까? >> Cluseter 유형의 리소스 자체가  클러스터 전역적으로 존재하고 관리되도록 설계되었기 때문


								: Namespace를 삭제하면 그 안의 리소스들도 함께 모두 삭제된다.


								: 기능
									1. 리소스 격리 >> 하나의 클러스터 내에서 여러 개발자/어플리케이션이 서로 간섭하지 않고 리소스를 관리할 수 있게 해줌

									2. 중복 이름 사용 >> ( 동일 클러스터 내에 그냥은 동일한 이름의 리소스가 있을 수 없지만) 서로 다른 Namesapce 에 속한 리소스들 간에는 동일한 이름을 사용 가능

									3. 자원 사용 제한 설정 >> ResourceQuota 리소스와 LimitRange 리소스를 통해 각 개발자/어플리케이션에 할당할 수 있는 리소스(CPU, 메모리, 스토리지.. ) 양을 제한할 수 있음

									4. 네트워크 정책 적용 >> NetworkPolicy를 사용하여 "특정 Namespace 내" 혹은 "Namespace 들 간"의 네트워크 트래픽을 제한 가능
										: 보안 정책을 세밀하게 적용 가능

									5. 접근 제어 >> Role-Based Access Control (RBAC) 를 사용하여 각 Namesapce 에 대해 접근 권한을 설정 가능
										: 특정 개발자/애플리케이션만 특정 Namespace 내의 리소스에만 접근 가능하도록 제어 가능


								: 기본적으로 존재하는 Namespace들
									(1) default >> 아무 Namespace도 지정하지 않을 때 사용하는 기본 Namespace
									(2) kube-system >> 쿠버네티스 자체 컴포넌트들이 실행되는 Namespace
									(3) kube-public >> 클러스터 내 모든 사용자에게 공개된 리소스가 포함된 Namespace
									(4) kube-node-lease >> 노드의 상태와 관련된 정보를 저장하는 데 사용되는 Namespace


	
						(2) 그외의 추가적인 오브젝트들 >> ConfigMap, Secret, ResourceQuota , LimitRange
							1. ResourceQuota 리소스 쿼터 >> 특정 Namespace가 사용/요청 가능한 최대 리소스 양을 설정하는 오브젝트
								: ResourceQuota를 설정함으로써, 특정 Namespace가 클러스터 전체 리소스를 과도하게 사용하는 것을 방지 가능

								: 동작 방식	
									: 리소스 생성 시 체크 >> 해당 Namespace의 사용자가 리소스를 생성하려고 할 때, 쿠버네티스는 ResourceQuota에 정의된 제한을 확인
										: 체크 했는데 제한 초과 안된 경우 >> 해당 Namespace 에서의 리소스 생성을 허용

										: 체크 했는데 제한 초과 시 >> 쿠버네티스는 이를 허용하지 않고 에러를 반환
	


							2. LimitRange 리밋 레인지 >>  특정 Namespace 내의  Pod/Container기 사용/요청 가능한 최대/최소 리소스 양을 설정하는 오브젝트
								: 각 Pod 또는 Container가 너무 적거나 너무 많은 리소스를 사용하지 않도록 제한해준다.




			:  controller >> "특정 타입의 리소스"의 "상태"를 (사용자의 의도대로 작동하도록) 제어/관리. 
				: 특정 리소스의 상태를 지속적으로 감시하고, 클러스터의 현재 상태와 사용자가 원하는 상태 간의 차이를 조정한다
					ex) 사용자가 "레플리카셋(ReplicaSet)을 통해 3개의 파드를 유지하고 싶다"고 지정한 경우  -> 레플리카셋 컨트롤러는 실제로 3개의 파드가 실행되고 있는지 확인하고, 만약 3개보다 적거나 많다면 이를 조정


		
				: 스프링 부트의 컨트롤러( url 매핑 따라 메서드 실행)랑은 다른 개념
					: kubernetes에서의 컨트롤러는, 말 그대로 (클러스터의 리소스를) "컨트롤" 한다는 의미에서 컨트롤러라고 이름이 붙여진 거임

				: 주요 컨트롤러
					1. 디플로이먼트 Deployment 컨트롤러 >> Deployment 리소스를 관리하는 컨트롤러
					2. 서비스 Service 컨트롤러 >> servie 리소스를 관리하는 컨트롤러
					3. 레플리카셋 ReplicaSet 컨트롤러 >> 지정된 수의 파드가 항상 실행되도록 관리


				: 컨트롤러는 API 서버와 상호 작용하여 작동
					:  그러니까 컨트롤러가 (클러스터의 상태를 모니터링하고 조정하는 데) 필요한 모든 정보를 API 서버를 통해 얻고, 조정 작업도 API 서버를 통해 수행한다






	Kubernetes Network  
	
		(1) Container to Container. 서로 결합된 컨테이너와 컨테이너 간 통신  >> 
		(2) Pod와 Pod 간의 통신
		(3) Pod와 Service간의 통신
		(4) 외부와 Service간의 통신





				: CNI Container Network Interfaces 컨테이너 네트워크 인터페이스
					: 쿠버네티스 클러스터에 존재하는 컨테이너 간의 통신을 가능하게 함

					: Master node 와 Worker node 의 유기적 통신을 가능하게 함
					: CNI 의 사용을 위해 Kubernetes network Plugin 을 제공
						:  Kubernetes network Plugin  >> 쿠버네티스에서 CNI의 사용을 가능하게 하는 플러그인. 
							: 주요 Kubernetes network Plugin  >> Flannel, calico
 								: 실습에서는 (보안이 뛰어나고 기능이 다양한) calico 를 사용


		: https://medium.com/finda-tech/kubernetes-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EC%A0%95%EB%A6%AC-fccd4fd0ae6


