chap 7.  쿠버네티스의 기본 구조
	- 쿠버네티스는 도커보다 규모가 커서 좀 더 빡세다

----------------------------------------------------------------------------------------------


Bare Metal 베어 메탈
	: 하드웨어 상에 어떤 소프트웨어도 설치되지 않은 썡 상태
	: 베어 메탈에 운영체제가 설치 된 것 == 우리가 아는 일반적인 컴퓨터



가상화 virtualiztion >> 하나의 리소스(물리적인 하드웨어)를 논리적으로 여러 리소스로 분할하는 기술
	: 하나의 하드웨어에서 여러개의 시스템을 효율적으로 사용할 수 있게 해준다
	: 가상화에서, 실제 하드웨어 == host 이고 가상머신 == Guest 이다
	: 종류
	   : 쉽게 정리하자면 , OS 수준까지 분리 할건지 VS  host OS는 공유하면서 프로세스만 분리할 것인지
		1. 가상머신 vm 을 통한 가상화 
		    : 가상머신 >> 하이퍼바이저를 이용해, 리소스 "전체"를 가상화한 것.
			: 가상머신들은 각각 다른 OS 를 가질 수 있다.  거의 물리적인 컴퓨터와 같은 기능을 한다
			: 하이퍼바이저 hypervisor >> 가상 머신을 생성 및 활용가능하게 하는 소프트웨어
				: VM과 하드웨어간의 IO명령을 처리하는 인터페이스다 >> 생성한 가상머신의 명령어를 (물리적인) 하드웨어가 이해할 수 있도록 번역해주어, 가상머신을 사용 가능하게 해준다
					: os 별로 사용하는 명령어가 다르기 때문에, 가상머신과 하드웨어의 os 가 다르다고 할 때, 그냥 가상머신의 명령어를 그대로 (물리적인) 하드웨어에서 실행하면 해당 하드웨어는 이게 뭔 개소린지 못알아먹는다 
					: 가상머신을 생성할 때 뿐 아니라, 생성한 가상머신을 사용만 할 때에도 반드시 하이퍼바이저를 거쳐야된다.

				: 참고로  가상머신 vm 통한 가상화 방식도 세부 종류 
				    :  일반적으로 가상머신을 통한 가상화라고 하면 호스트 가상화
					(1) 호스트 가상화 >> host os 위에 올라가는 하이퍼바이저
					(2) 하이퍼바이저 가상화 >> host(하드웨어)위 에 바로 올라가는 하이퍼바이저 (host os 가 따로 없다)


			: 가상머신을 사용한 가상화를 할 경우 용량도 크고 속도도 느리다
		

		2. 컨테이너 container 를 통한 가상화
		    : 컨테이너 >> 필요한 프로세스를 표준화된 방식으로 패키징해 격리해 둔 것.
			: 프로세스만 격리한 것으로, 각 컨테이너는  host OS를 공유하게 된다.
				: host OS 를 공유한다
				== 커널을 공유한다 >> 호스트 OS 의 기능을 모두 사용 가능하다
				== 서로 다른 컨테이너끼리 통신을 주고 받을 수 있다
			: 하드웨어위에 host os 가 올라가고, 그 위에 docker 와 같은 컨테이너 응용 프로그램이 올라가고, 그 프로그램 위에 컨테이너들이 올라간다
			:  (걍 화물선에 무작위로 적재하는게 아니라) 표준화된 방식으로 패키징(컨테이너 박스)해서, 컨테이너 엔진(표준화된 운송선)만 있으면 돌아갈 수 있게하는거다.
			: 하이퍼바이저와 guest os 를 따로 필요로 하지 않아 상대적으로 가볍고 빠르다	
			: 컨테이너는 마이크로서비스아키텍쳐 MSA 의 기반이 된다. 
	: https://worlf.tistory.com/141






Container 기술
	: 왜 씀?
		1. 각 애플리케이션을 격리하여 종속성 충돌 문제를 방지
			: 베어메탈(쌩 컴퓨터. 가상머신 같은거 없는) 환경에서는 애플리케이션을 호스트 운영체제에 직접 설치하고 구성하여 애플리케이션 간 종속성 충돌 문제 발생 가능

		2. 가상화 기술을 사용하여 호스트 시스템 리소스를 효율적으로 공유 가능
			: host OS 위에 애플리케이션 프로세스의 형태로 격리한거니까

	: (Linux 운영 체제의 핵심 기능인) Linux namespaces와 cgroups를 기반으로 함
	: (컨테이너 관리를 위한) 주요 컨테이너 도구 >> Docker 등
	: Container 의 역사
		: 1979년, 컨테이너 개념 등장.  ‘chroot’ 시스템 호출의 도입.
			  chroot >> 프로세스와 그 자식 프로세스의 루트 디렉터리를 파일 시스템 내에서 새로운 위치로 변경하는 것으로, 프로세스 격리(Isolation)의 시작

		: 2000년, FreeBSD Jails가 개발
			FreeBSD Jails >> FreeBSD 시스템을 여러 독립적인 작은 “감옥(Jails)”으로 분할하고 각각에게 IP 주소를 할당

		: 2001년, Jail 메커니즘인 Linux VServer가 개발
			Linux VServer >> FreeBSD Jails와 유사한 원리로, 컴퓨터 시스템의 리소스를 분할하는 것을 가능하게함.  Linux 커널을 패치하여 구현됨

		: 2006년, 구글에서 Process Containers개발
			:  Process Containers >> 프로세스 집합의 리소스 사용량(CPU, 메모리, 디스크 I/O, 네트워크)을 제한하고 격리하기 위해 설계됨
				: 2007년에는  Process Containers가 ‘Control Groups(cgroups)’로 이름이 바뀌었으며, Linux 커널 2.6.24에 통합됨

		: 2008년, LXC 가 개발됨
			LXC (LinuX Containers) >> cgroups와 Linux namespaces를 사용하여 구현한 최초의 Linux 컨테이너 엔진으로, Linux 커널을 패치하지 않고도 동작

		: 2013년, LMCTFY(Let Me Contain That For You)가 시작
			LMCTFY(Let Me Contain That For You) >> Linux 애플리케이션 컨테이너를 제공하는 구글 컨테이너 스택의 오픈 소스 버전. 애플리케이션은 “컨테이너 인식”을 통해 하위 컨테이너를 만들고 관리 가능. 
				: 2015년부터 LMCTFY의 핵심 개념은 Open Container Foundation의 일부인 libcontainer로 이전시키기 시작됬으며, 이후 LMCTFY의 배포는 중단됨


		: 2013 년, Docker 가 등장하며 컨테이너 기술이 큰 인기를 얻음
			: 초기 Docker 는 LXC를 기반으로 했지만, 나중엔 이를 자체 라이브러리인 libcontainer 로 교체함
				: LXC 와 libcontainer 
					: LXC >> 전체 가상화 기반. 주로 서버 환경에서 사용됨. 명령줄 도구를 통해 컨테이너를 관리함
					: libcontainer >> OS 수준의 컨테이너화를 가능하게 하는 라이브러리. 주로 간단한 컨테이너 관리를 위해 Docker 와 같은 컨테이너 관리 도구에 내장되어 활용됨





쿠버네티스 Kubernetes >> 컨테이너화된 애플리케이션의 자동 배포, 확장 및 관리를 해주는 오픈소스 opensource 플랫폼
	Kubernetes 어원 
		: Kubernetes 자체는 고대 그리스어로, 배의 조타수(Helmsman)를 의미
		: Kubernetes 를 K8s 라고도 부름
			: Kubernetes단어가 K 와 s 사이에 8 글자로 이뤄져서 그렇다

	Kubernetes의 역사
	    : 요약 >>
 		: Kubernetes는 2014 년 발표된 구글의 사내 프로젝트. 
			: Borg 의 영향을 많이 받음
			: Borg >> 수천~수백 만개의 작업을 실행/관리하는 구글 사내 시스템

		: 2016 년엔 Helm이 발표됨
			: Helm 헬름>> 쿠버네티스 패키지 관리 프로그램
				: 이후 chap9에서 자세히 다룸


	    : Kubernetes는 Container mangement system  중 하나
			: https://tech.osci.kr/kubernetes-%ED%83%84%EC%83%9D-%EB%B0%B0%EA%B2%BD-%EB%B9%84%ED%95%98%EC%9D%B8%EB%93%9C-%EC%8A%A4%ED%86%A0%EB%A6%AC/

			: Container mangement system >> 컨테이너 관리 시스템
				: 구글의 대규모 데이터 센터에서 효율적 작업을 위해 개발
				; 주요 오픈소스 Container mangement system 
					1. Borg >>컨테이너를 사용하여 prod ( 장기 실행 서비스 )와 batch(non-prod)작업을 효과적으로 관리하는 시스템
						: 구글 최초의 통합 컨테이너 관리 시스템
						: Linux 컨테이너 기술 기반

						:  클러스터 Cluster 와 셀 cell <<데이터 센터 장비들의 집합 단위
							Cluster >>  군집. 한 데이터센터내에서 서로 네트워크가 연결된 장비들.
								: 1개 이상의 셸cell 로 이루어짐

							셀 cell >> 여러개의 장비들을 하나로 묶은, Cluster 를 관리하기 위한 단위로, 보통 10000 대 정도의 장비를 의미
								: Cell 에서는 여러 Task가 돌아감	
								: 보통 수천명의 사용자가 하나의 borg 셀을 공유해서 사용



						: job과 Task 
							Task >> Borg 시스템에서 실행되어야 하는 개별 작업 단위. 컨테이너에서 실행되는 리눅스 프로세스들.
								: 분류
									(1) long-running 서비스(prod) >> 오래 돌아가는, "절대" 죽으면 안되는 서비스.
									(2) batch 작업(non-prod) >> 일시적으로 돌아가는 서비스



							Job>>  작업 그룹 . 여러 TASK를 그룹화한 단위
								priority >> 우선순위. 셀 내에서 실행/대기중인 JOB의 상대적 중요도.
									: 모든 Job 엔 priority 가 있다
									: 처리 가능한 양보다 많은 Task , JOB이 셸에 할당 될 경우 쓰임

								quota 쿼터>> Job 에서 사용될 수 있는 자원의 최대치 값
									: 어떤 잡이 스케줄링 가능한지 결정하는데 쓰임
									: CPU, RAM, 디스크 등으로 표현됨
									


						: alloc	 와 alloc set 
							 : 일반적으로 task 는 alloc 를 참조, job 은 alloc set 을 참조
							alloc >> 하나 이상의 TASK 들에 대한, 장비에 미리 할당해 둔 자원들 . 
							alloc set >> 여러 alloc들을 그룹화한 단위
 			
						:  BorgMaster >> Borg 시스템의 중앙 제어 역할. 
							: 사용자가 시스템과 상호 작용할 수 있는 API를 제공하고, 작업을 예약하고 스케줄링하며 클러스터 관리를 수행
							: 구성 >> 메인 borgmaster 프로세스 + 2개의 스케쥴러 프로세스
								: 스케쥴링하는데 꽤 오래 걸리기 떄문에 빠른 처리를 위해 borgmaster의 스케쥴러는 다른 기능들과 분리되어있음
								
								:  메인 borgmaster 프로세스 
									: 역할
										1. Borg 시스템내부의 모든 객체(장비, 태스크, alloc등)의 상태를 관리.
										2. Borglet 과 통신
										3. 컨트롤러(클라이언트 request 처리)

									: 논리적으로는 단일 프로세스지만 실제로는 5개의 사본을 가짐
										: 사본 >> 셸의 상태를 메모리에 복제해서 관리함
							: 하나의 borgmaster는 셀내의 수천개의 장비를 관리 가능

							: checkpoint >> Borgmaster의 특정 시점에서의 상태
								: paxos 저장소에 기간별 스냅샷으로 보관됨


						: 스케쥴러 scheduler >> Job의 자원조건 보고, 적당한 자원을 가진 장비에 Task를 할당
							: 작동 절차
								1. Job 이 제출되면 Borgmaster는 그걸 paxos 저장소에 저장, Job의 Task를 pending 큐에 추가
								2. scheduler가 pending 큐를 비동기적으로 확인해 적절한 장비에 Task 를 할당

							: 주로 Task 를 관리하는거지, Job을 관리하는게 아니다
							: 높은 우선순위를 가진 Task 가 먼저 스케쥴링 된다


						: Borglet >> 셀내의 모든 장비들에서 돌아가는 Borg 에이전트
							: 역할 
								1. borgmaster와 해당 장비가 통신 가능하게 함											:  borgmaster나 모니터링 시스템에 장비 상태 보고
		
								2. 태스크의 시작/정지. 실패했을때 재시작등을 담당
								3. 로컬자원 관리
							: borgmaster에 요청이 몰리는걸 막기 위해, borgmaster에서 몇 초 간격으로 borglet에서 장비의 상태를 요청한다
								: 이거 SMTP Simple Mail Transfer Protocol아님?


						: Borg 와 Kubernetes 
							: Borg 의 단점 
								1. Job은 Task 를 그룹화 하는 용도로만 사용 가능
									: Borg에선 멀티 Job을 하나로 처리하거나, 잡의 특정 하위집합만 묶어서 처리할 수 없어서 롤링 업데이트나 잡 크기변경등을 못한다.
									: Kubernetes는 label을 이용해서 pods들을 스케쥴링한다.  작업할 대상을 정한다.


								2. 장비당 하나의 IP 주소만 있어서 처리하기가 복잡
									: borg에서는 장비에 IP가 하나밖에 없어서 각 Task들이 장비의 포트를 구분해서 사용해야됨. 
										: 포트를 자원으로 간주하고 관리하는게 복잡했음 
											: 태스크가 얼마나 많은 포트를 사용할 지 등을 미리 정해놔야됨
									: Kubernetes는 모든 pod 와 서비스들이 각각의 IP 주소를 가짐. 포트를 관리할 필요가 없음


							: Borg 의 장점
								1. alloc은 유용
									: borg에서는 alloc을 이용해서 부가 서비스(웹서버가 사용하는 데이터를 주기적으로 업데이트하는 등)들을 별도의 팀에서 개발하는게 가능
									: kubernetes 에서는 alloc 와 유사한 개념으로 pod 가 있음(같은 pod내에 헬퍼 컨테이너를 사용가능)

								2. 클러스터 관리라는게 Task 관리만 하는건 아니다
									: borg에선 Task와 장비의 생명주기를 관리하는 것 뿐 아니라 네이밍, 로드밸런싱등의 서비스 제공
									: Kubernetes는 service를 이용해서 네이밍과 로드밸런싱을 지원
										: 네이밍 >> label selector를 통해서 pod에 동적으로 서비스 이름을 정의 가능. 그리고 정의된 클러스터내의 모든 컨테이너들은 서비스명을 기반으로 서비스에 연결 가능
										: 로드 밸런싱 >>  자동으로 서비스에 대한 커넥션들을 pod에 로드밸런싱

								3. Introspection이 중요하다
									: borg 에선 디버깅 정보를 모든 사용자가 볼 수 있다. 
									: Kubernetes는 borg의 introspection 테크닉을 많이 가져옴(
자원 모니터링을 위해 cAdvisor를 사용하고, Elasticsearch/Kibana/Fluentd 를 이용해 로그를 수집.  ) 마스터는 객체의 상태에 대한 스냅샷을 요청가능.  모든 구성요소들이 사용할 수 있는 이벤트(pod가 스케쥴되었는지, 컨테이너가 장애인지등)를 기록하는 표준화된 구조를 가지고 있고, 이걸 클라이언트가 이용할 수 있다.


								4. 마스터는 분산 시스템의 커널
									; Borgmaster는 처음에는 단일 프로세스로 시작했지만, 시간이 흐르면서 스케쥴러나 UI 그외 기능등을 별도의 프로세스로 분리해 내면서 스케일업하기에 좋은 구조로 발전

									: Kubernetes는 여기서 한발 더 나아가서 API 서버를 가짐 (클러스터 관리에 필요한 기능들을 마이크로 서비스로 만들어서 API 서버의 클라이언트가 되게 만들었다)


						: https://arisu1000.tistory.com/27785

					2. Omega >>  Borg 시스템을 개선시킨 시스템
						: 구글 두번째 컨테이너 관리 시스템. 
						: Omlet >> 애플리케이션을 컨테이너화하고 실행하는 작은 실행 단위.
							: 각 Omlet 은 개별적으로 구성되며, 컨테이너 클러스터 내에서 관리됨						: 클러스터에서 작업을 수행하는 데 사용되는 컨테이너. 

						: Cell >> 대규모 머신들을 관리하는 단위. 

						: CellState >> Omlets의 정보를 실시간으로 추적 
						: Cell Service >> Omlets이 필요로 하는 서비스 제공

					3. Kubernetes >> Omega, Borg 시스템을 개선한 시스템
						: 구글 세번째 컨테이너 관리 시스템. 
						: 구글의 최신 프로그래밍 언어인 Go를 기반으로 작성됨
						: 개발자가 클러스터에서 실행되는 복잡한 분산 시스템을 쉽게 배포하고 관리하기 위해 설계됨

	
	Kubernetes란? 역할은?
		: 수 많은 컨테이너를 쉽게 관리하게 해주는 시스템
			: 걍 생각해봐도 100 개의 컨테이너가 있을 때, docker container run 을 100 번 치는건 좀.. 음.. 개노답

		: 서버를 다수 운영하는 상황에서, 서로 다른 서버에서 작동하는 컨테이너를 한꺼번에 쉽게 관리 가능
 
	Kubernetes의 구조
		: 도커에 비해 좀 더 구성이 복잡
		: 구성 요소
			(1) 쿠버네티스 클러스터
				: 종류
					1. Master node 마스터 노드 
						: 개발자가 주로 통신하게 되는 노드
							: 개발자의 API 요청을 받음
						:  Master node에서 Control Plane 을 다룸. 
							: Control Plane 을 제어 가능하다는 거임 아님 Master node 의 구성 요소가 Control Plane 이라는 거임?

						: 뇌피셜 - worker node 에 접근 가능한 듯

					2. Worker node 워커 노드
						: 클라이언트가 주로 통신/이용하게 되는 노드. 
							: 인터넷을 통한 클라이언트 트래픽을 분할 받음
						: 애플리케이션이 올라감

				: CNI Container Network Interfaces 컨테이너 네트워크 인터페이스
					: 쿠버네티스 클러스터에 존재하는 컨테이너 간의 통신을 가능하게 함
						: node가 컨테이너임?

					: Master node 와 Worker node 의 유기적 통신을 가능하게 함
					: CNI 의 사용을 위해 Kubernetes network Plugin 을 제공
						:  Kubernetes network Plugin  >> 쿠버네티스에서 CNI의 사용을 가능하게 하는 플러그인. 
							: 주요 Kubernetes network Plugin  >> Flannel, calico
 								: 실습에서는 (보안이 뛰어나고 기능이 다양한) calico 를 사용

			(2) Control Plane 컨트롤 플레인 >> 쿠버네티스 클러스터 전반의 작업을 관리
				: 쿠버네티스의 작업 >>  kubectl 명령어로 master node 의 kube-apiserver 에게 API 요청을 보냄으로 써 이루어딤

				: 구성 요소 
					(1) API 서버 >> 쿠버네티스 control plane에서의 프론트앤드 역할을 함
						: 그러니까 개발자의 request 를 받는, controller 단이라는 건가?
 
					(2) ectd >> 쿠버네티스 클러스터 상의 모든 데이터를 key-value꼴로 저장하는 저장소
					(3) kube-scheduler 스케쥴러 >> 새롭게 생성되는 pod 를 어느 노드에 실행시킬지 정함
						: 쿠버네티스에서는 pod 라는 오브젝트를 통해 애플리케이션을 실행하게 됨
							: pod 는 이후 배운다
	 
					(4) 컨트롤러 매니저 >> 쿠버네티스 리소스를 관리하고 제어하는 역할
						: 컨트롤러 >> 클러스터 상태를 모니터링
							: 마스터 노드에서 실행됨
							: 컨트롤러로는 여러 종류가 있고, 각 컨트롤러는 특정 리소스 타입을 관리
								1. 디플로이먼트 Deployment 컨트롤러
								2. 서비스 Servie 컨트롤러
								3. 레플리카셋 ReplicaSet 컨트롤러


			(3) 노드
				: 구성 요소
					1. Kublet >> pod 내부의 컨테이너 실행을 담당
						: 쿠버네티스 클러스터의 각 노드에는 kublet 이 실행됨
						: pod 의 상태를 모니터링 하고, pod 상태에 이상이 있을 시 해당 pod 를 다시 배포

					2. Kube-Proxy >> node 에서 network 역할을 수행
						: node 에 존재하는 pod 들이 쿠버네티스 내부/외부와의 네트워크 토신을 가능하게 함
							: 아니 그럼 network 자체의 역할이라기 보다는 network 인터페이스의 역할을 수행한다고 말해야되는거 아님?

					3. Container runtime 컨테이너 런타임>> 컨테이너의 생명주기를 담당
						:  Container runtime interface 컨테이너 런타임 인터페이스>> Kublet 과 Container runtime 간의 통신을 가능하게 함
						: 대표적인 container runtime >> containerd, CRI-O
							: 실습에서는 가장 보편적인 containerd 를 사용

				: pod 파드 >> 컨테이너를 실행하기 위한 오브젝트
					: 쿠버네티스에서는 컨테이너가 pod 내에서 실행된다
						: 도커에서는 컨테이너가 단독으로 실행되었다
					: 각 pod 는 1개 이상의 컨테이너를 담을 수 있다
					: 그러니까 pod == 컨테이너를 그룹화 한 것
						: 도커에서는 최소 실행단위가 컨테이너 였다면, 쿠버네티스의 최소 실행 단위는 pod 라고 봄 된다
					: 다수의 pod 들은 여러 워커 노드에 분산되어 실행됨
						: 그러니까 서로 다른 pod 들은 서로 다른 노드에서 실행될 수 있지만, 하나의 pod 가 분할 되어 여러 노드에 실행되는 일은 없다는 것 
							: 다시한번 말하지만 쿠버네티스의 최소 실행단위는 pod 이다

					: 하나의 pod 에 속하는 컨테이너들은 하나의 목적을 위해 구성된 컨테이너들임. 
					: pod 는 컨테이너처럼 일시적인 존재
						: pod 는 실행할 때마다 IP주소를 배정받으므로, pod 의 IP 주소는 실행할 때마다 변경됨


			(4) 워크 로드 workload >> 쿠버네티스에서 실행되는 애플리케이션
				: 하나의 컴포넌트 형태로 실행하든, 다수의 컴포넌트가 함께 실행하든 쿠버네티스는 pod 내부에서 워크 로드를 실행하게 됨
					: 이때 pod == 실행중인 컨테이너의 집합

				: 종류
					1. Replicaset 레플리카셋>> pod 의 복제를 관리
						: 클라이언트가 요구하는 복제본 개수만큼 pod 를 복제 및 모니터링

					2. Deployment 디플로이먼트 >> 배치. 애플리케이션의 배포와 스케일링을 관리
					3. StatefulSet 스테이트풀셋 >> pod 사이에서 순서와 고유성이 보장되어야하는 경우 사용
					4. DaemonSet 데몬 셋 >> 쿠버네티스를 구성하는 모든 노드가 pod의 복사본을 실행하게 함
						: 쿠버네티스 클러스터에 새로운 노드가 추가되면 pod 역시 추가됨 
						: 주로 (로깅, 모니터링, 스토리지 와 같은) 시스템 수준의 서비스를 실행하는데 사용됨

					5. Job 과 Cronjob >> Task 가 정상적으로 완료/종료되는 것을 담당
						: pod 가 정상적으로 종료되지 않는다면 재실행시킴
						: Job >> 작업이 한 번 종료 되는 것을 담당
						: Cronjob >> 리눅스의 Crontab 과 비슷한 역할
				

 			(5) 네트워크 
				1. 서비스 service 
					: pod 를 여러개 묶어서 클러스터 외부로 노출시킬 수 있게 함
					: 장점 >> 이미 실행 중인 pod 를 외부로 노출시키기 위해 pod 내부를 수정할 필요가 없다. 쿠버네티스 서비스를 활용하면 실행중인 pod 의 수정 없이도 외부에 노출 시켜 클라이언트와의 통신이 가능하다

				2. 인그레스 Ingress 
					: 쿠버네티스 내부에 존재하는 서비스를 HTTP/HTTPS 루트를 클러스터 외부로 라우팅 하는 역할

			(6) 스토리지 
				: 컨테이너 내부 존재하는 파일들은 수명이 짧다 but 쿠버네티스 스토리지를 활용하면 pod 의 상태와 상관 없이 파일을 보관할 수 있다
					: 컨테이너 내부 존재하는 파일들은 수명이 짧다는 의미 >> 컨테이너가 삭제/재실행되면 해당 컨테이너 내부의 파일들은 모두 삭제되니까
chap 7.  쿠버네티스의 기본 구조
	- 쿠버네티스는 도커보다 규모가 커서 좀 더 빡세다

----------------------------------------------------------------------------------------------


Bare Metal 베어 메탈
	: 하드웨어 상에 어떤 소프트웨어도 설치되지 않은 썡 상태
	: 베어 메탈에 운영체제가 설치 된 것 == 우리가 아는 일반적인 컴퓨터



가상화 virtualiztion >> 하나의 리소스(물리적인 하드웨어)를 논리적으로 여러 리소스로 분할하는 기술
	: 하나의 하드웨어에서 여러개의 시스템을 효율적으로 사용할 수 있게 해준다
	: 가상화에서, 실제 하드웨어 == host 이고 가상머신 == Guest 이다
	: 종류
	   : 쉽게 정리하자면 , OS 수준까지 분리 할건지 VS  host OS는 공유하면서 프로세스만 분리할 것인지
		1. 가상머신 vm 을 통한 가상화 
		    : 가상머신 >> 하이퍼바이저를 이용해, 리소스 "전체"를 가상화한 것.
			: 가상머신들은 각각 다른 OS 를 가질 수 있다.  거의 물리적인 컴퓨터와 같은 기능을 한다
			: 하이퍼바이저 hypervisor >> 가상 머신을 생성 및 활용가능하게 하는 소프트웨어
				: VM과 하드웨어간의 IO명령을 처리하는 인터페이스다 >> 생성한 가상머신의 명령어를 (물리적인) 하드웨어가 이해할 수 있도록 번역해주어, 가상머신을 사용 가능하게 해준다
					: os 별로 사용하는 명령어가 다르기 때문에, 가상머신과 하드웨어의 os 가 다르다고 할 때, 그냥 가상머신의 명령어를 그대로 (물리적인) 하드웨어에서 실행하면 해당 하드웨어는 이게 뭔 개소린지 못알아먹는다 
					: 가상머신을 생성할 때 뿐 아니라, 생성한 가상머신을 사용만 할 때에도 반드시 하이퍼바이저를 거쳐야된다.

				: 참고로  가상머신 vm 통한 가상화 방식도 세부 종류 
				    :  일반적으로 가상머신을 통한 가상화라고 하면 호스트 가상화
					(1) 호스트 가상화 >> host os 위에 올라가는 하이퍼바이저
					(2) 하이퍼바이저 가상화 >> host(하드웨어)위 에 바로 올라가는 하이퍼바이저 (host os 가 따로 없다)


			: 가상머신을 사용한 가상화를 할 경우 용량도 크고 속도도 느리다
		

		2. 컨테이너 container 를 통한 가상화
		    : 컨테이너 >> 필요한 프로세스를 표준화된 방식으로 패키징해 격리해 둔 것.
			: 프로세스만 격리한 것으로, 각 컨테이너는  host OS를 공유하게 된다.
				: host OS 를 공유한다
				== 커널을 공유한다 >> 호스트 OS 의 기능을 모두 사용 가능하다
				== 서로 다른 컨테이너끼리 통신을 주고 받을 수 있다
			: 하드웨어위에 host os 가 올라가고, 그 위에 docker 와 같은 컨테이너 응용 프로그램이 올라가고, 그 프로그램 위에 컨테이너들이 올라간다
			:  (걍 화물선에 무작위로 적재하는게 아니라) 표준화된 방식으로 패키징(컨테이너 박스)해서, 컨테이너 엔진(표준화된 운송선)만 있으면 돌아갈 수 있게하는거다.
			: 하이퍼바이저와 guest os 를 따로 필요로 하지 않아 상대적으로 가볍고 빠르다	
			: 컨테이너는 마이크로서비스아키텍쳐 MSA 의 기반이 된다. 
	: https://worlf.tistory.com/141






Container 기술
	: 왜 씀?
		1. 각 애플리케이션을 격리하여 종속성 충돌 문제를 방지
			: 베어메탈(쌩 컴퓨터. 가상머신 같은거 없는) 환경에서는 애플리케이션을 호스트 운영체제에 직접 설치하고 구성하여 애플리케이션 간 종속성 충돌 문제 발생 가능

		2. 가상화 기술을 사용하여 호스트 시스템 리소스를 효율적으로 공유 가능
			: host OS 위에 애플리케이션 프로세스의 형태로 격리한거니까

	: (Linux 운영 체제의 핵심 기능인) Linux namespaces와 cgroups를 기반으로 함
	: (컨테이너 관리를 위한) 주요 컨테이너 도구 >> Docker 등
	: Container 의 역사
		: 1979년, 컨테이너 개념 등장.  ‘chroot’ 시스템 호출의 도입.
			  chroot >> 프로세스와 그 자식 프로세스의 루트 디렉터리를 파일 시스템 내에서 새로운 위치로 변경하는 것으로, 프로세스 격리(Isolation)의 시작

		: 2000년, FreeBSD Jails가 개발
			FreeBSD Jails >> FreeBSD 시스템을 여러 독립적인 작은 “감옥(Jails)”으로 분할하고 각각에게 IP 주소를 할당

		: 2001년, Jail 메커니즘인 Linux VServer가 개발
			Linux VServer >> FreeBSD Jails와 유사한 원리로, 컴퓨터 시스템의 리소스를 분할하는 것을 가능하게함.  Linux 커널을 패치하여 구현됨

		: 2006년, 구글에서 Process Containers개발
			:  Process Containers >> 프로세스 집합의 리소스 사용량(CPU, 메모리, 디스크 I/O, 네트워크)을 제한하고 격리하기 위해 설계됨
				: 2007년에는  Process Containers가 ‘Control Groups(cgroups)’로 이름이 바뀌었으며, Linux 커널 2.6.24에 통합됨

		: 2008년, LXC 가 개발됨
			LXC (LinuX Containers) >> cgroups와 Linux namespaces를 사용하여 구현한 최초의 Linux 컨테이너 엔진으로, Linux 커널을 패치하지 않고도 동작

		: 2013년, LMCTFY(Let Me Contain That For You)가 시작
			LMCTFY(Let Me Contain That For You) >> Linux 애플리케이션 컨테이너를 제공하는 구글 컨테이너 스택의 오픈 소스 버전. 애플리케이션은 “컨테이너 인식”을 통해 하위 컨테이너를 만들고 관리 가능. 
				: 2015년부터 LMCTFY의 핵심 개념은 Open Container Foundation의 일부인 libcontainer로 이전시키기 시작됬으며, 이후 LMCTFY의 배포는 중단됨


		: 2013 년, Docker 가 등장하며 컨테이너 기술이 큰 인기를 얻음
			: 초기 Docker 는 LXC를 기반으로 했지만, 나중엔 이를 자체 라이브러리인 libcontainer 로 교체함
				: LXC 와 libcontainer 
					: LXC >> 전체 가상화 기반. 주로 서버 환경에서 사용됨. 명령줄 도구를 통해 컨테이너를 관리함
					: libcontainer >> OS 수준의 컨테이너화를 가능하게 하는 라이브러리. 주로 간단한 컨테이너 관리를 위해 Docker 와 같은 컨테이너 관리 도구에 내장되어 활용됨





쿠버네티스 Kubernetes >> 컨테이너화된 애플리케이션의 자동 배포, 확장 및 관리를 해주는 오픈소스 opensource 플랫폼
	Kubernetes 어원 
		: Kubernetes 자체는 고대 그리스어로, 배의 조타수(Helmsman)를 의미
		: Kubernetes 를 K8s 라고도 부름
			: Kubernetes단어가 K 와 s 사이에 8 글자로 이뤄져서 그렇다

	Kubernetes의 역사
	    : 요약 >>
 		: Kubernetes는 2014 년 발표된 구글의 사내 프로젝트. 
			: Borg 의 영향을 많이 받음
			: Borg >> 수천~수백 만개의 작업을 실행/관리하는 구글 사내 시스템

		: 2016 년엔 Helm이 발표됨
			: Helm 헬름>> 쿠버네티스 패키지 관리 프로그램
				: 이후 chap9에서 자세히 다룸


	    : Kubernetes는 Container mangement system  중 하나
			: https://tech.osci.kr/kubernetes-%ED%83%84%EC%83%9D-%EB%B0%B0%EA%B2%BD-%EB%B9%84%ED%95%98%EC%9D%B8%EB%93%9C-%EC%8A%A4%ED%86%A0%EB%A6%AC/

			: Container mangement system >> 컨테이너 관리 시스템
				: 구글의 대규모 데이터 센터에서 효율적 작업을 위해 개발
				; 주요 오픈소스 Container mangement system 
					1. Borg >>컨테이너를 사용하여 prod ( 장기 실행 서비스 )와 batch(non-prod)작업을 효과적으로 관리하는 시스템
						: 구글 최초의 통합 컨테이너 관리 시스템
						: Linux 컨테이너 기술 기반

						:  클러스터 Cluster 와 셀 cell <<데이터 센터 장비들의 집합 단위
							Cluster >>  군집. 한 데이터센터내에서 서로 네트워크가 연결된 장비들.
								: 1개 이상의 셸cell 로 이루어짐

							셀 cell >> 여러개의 장비들을 하나로 묶은, Cluster 를 관리하기 위한 단위로, 보통 10000 대 정도의 장비를 의미
								: Cell 에서는 여러 Task가 돌아감	
								: 보통 수천명의 사용자가 하나의 borg 셀을 공유해서 사용



						: job과 Task 
							Task >> Borg 시스템에서 실행되어야 하는 개별 작업 단위. 컨테이너에서 실행되는 리눅스 프로세스들.
								: 분류
									(1) long-running 서비스(prod) >> 오래 돌아가는, "절대" 죽으면 안되는 서비스.
									(2) batch 작업(non-prod) >> 일시적으로 돌아가는 서비스



							Job>>  작업 그룹 . 여러 TASK를 그룹화한 단위
								priority >> 우선순위. 셀 내에서 실행/대기중인 JOB의 상대적 중요도.
									: 모든 Job 엔 priority 가 있다
									: 처리 가능한 양보다 많은 Task , JOB이 셸에 할당 될 경우 쓰임

								quota 쿼터>> Job 에서 사용될 수 있는 자원의 최대치 값
									: 어떤 잡이 스케줄링 가능한지 결정하는데 쓰임
									: CPU, RAM, 디스크 등으로 표현됨
									


						: alloc	 와 alloc set 
							 : 일반적으로 task 는 alloc 를 참조, job 은 alloc set 을 참조
							alloc >> 하나 이상의 TASK 들에 대한, 장비에 미리 할당해 둔 자원들 . 
							alloc set >> 여러 alloc들을 그룹화한 단위
 			
						:  BorgMaster >> Borg 시스템의 중앙 제어 역할. 
							: 사용자가 시스템과 상호 작용할 수 있는 API를 제공하고, 작업을 예약하고 스케줄링하며 클러스터 관리를 수행
							: 구성 >> 메인 borgmaster 프로세스 + 2개의 스케쥴러 프로세스
								: 스케쥴링하는데 꽤 오래 걸리기 떄문에 빠른 처리를 위해 borgmaster의 스케쥴러는 다른 기능들과 분리되어있음
								
								:  메인 borgmaster 프로세스 
									: 역할
										1. Borg 시스템내부의 모든 객체(장비, 태스크, alloc등)의 상태를 관리.
										2. Borglet 과 통신
										3. 컨트롤러(클라이언트 request 처리)

									: 논리적으로는 단일 프로세스지만 실제로는 5개의 사본을 가짐
										: 사본 >> 셸의 상태를 메모리에 복제해서 관리함
							: 하나의 borgmaster는 셀내의 수천개의 장비를 관리 가능

							: checkpoint >> Borgmaster의 특정 시점에서의 상태
								: paxos 저장소에 기간별 스냅샷으로 보관됨


						: 스케쥴러 scheduler >> Job의 자원조건 보고, 적당한 자원을 가진 장비에 Task를 할당
							: 작동 절차
								1. Job 이 제출되면 Borgmaster는 그걸 paxos 저장소에 저장, Job의 Task를 pending 큐에 추가
								2. scheduler가 pending 큐를 비동기적으로 확인해 적절한 장비에 Task 를 할당

							: 주로 Task 를 관리하는거지, Job을 관리하는게 아니다
							: 높은 우선순위를 가진 Task 가 먼저 스케쥴링 된다


						: Borglet >> 셀내의 모든 장비들에서 돌아가는 Borg 에이전트
							: 역할 
								1. borgmaster와 해당 장비가 통신 가능하게 함											:  borgmaster나 모니터링 시스템에 장비 상태 보고
		
								2. 태스크의 시작/정지. 실패했을때 재시작등을 담당
								3. 로컬자원 관리
							: borgmaster에 요청이 몰리는걸 막기 위해, borgmaster에서 몇 초 간격으로 borglet에서 장비의 상태를 요청한다
								: 이거 SMTP Simple Mail Transfer Protocol아님?


						: Borg 와 Kubernetes 
							: Borg 의 단점 
								1. Job은 Task 를 그룹화 하는 용도로만 사용 가능
									: Borg에선 멀티 Job을 하나로 처리하거나, 잡의 특정 하위집합만 묶어서 처리할 수 없어서 롤링 업데이트나 잡 크기변경등을 못한다.
									: Kubernetes는 label을 이용해서 pods들을 스케쥴링한다.  작업할 대상을 정한다.


								2. 장비당 하나의 IP 주소만 있어서 처리하기가 복잡
									: borg에서는 장비에 IP가 하나밖에 없어서 각 Task들이 장비의 포트를 구분해서 사용해야됨. 
										: 포트를 자원으로 간주하고 관리하는게 복잡했음 
											: 태스크가 얼마나 많은 포트를 사용할 지 등을 미리 정해놔야됨
									: Kubernetes는 모든 pod 와 서비스들이 각각의 IP 주소를 가짐. 포트를 관리할 필요가 없음


							: Borg 의 장점
								1. alloc은 유용
									: borg에서는 alloc을 이용해서 부가 서비스(웹서버가 사용하는 데이터를 주기적으로 업데이트하는 등)들을 별도의 팀에서 개발하는게 가능
									: kubernetes 에서는 alloc 와 유사한 개념으로 pod 가 있음(같은 pod내에 헬퍼 컨테이너를 사용가능)

								2. 클러스터 관리라는게 Task 관리만 하는건 아니다
									: borg에선 Task와 장비의 생명주기를 관리하는 것 뿐 아니라 네이밍, 로드밸런싱등의 서비스 제공
									: Kubernetes는 service를 이용해서 네이밍과 로드밸런싱을 지원
										: 네이밍 >> label selector를 통해서 pod에 동적으로 서비스 이름을 정의 가능. 그리고 정의된 클러스터내의 모든 컨테이너들은 서비스명을 기반으로 서비스에 연결 가능
										: 로드 밸런싱 >>  자동으로 서비스에 대한 커넥션들을 pod에 로드밸런싱

								3. Introspection이 중요하다
									: borg 에선 디버깅 정보를 모든 사용자가 볼 수 있다. 
									: Kubernetes는 borg의 introspection 테크닉을 많이 가져옴(
자원 모니터링을 위해 cAdvisor를 사용하고, Elasticsearch/Kibana/Fluentd 를 이용해 로그를 수집.  ) 마스터는 객체의 상태에 대한 스냅샷을 요청가능.  모든 구성요소들이 사용할 수 있는 이벤트(pod가 스케쥴되었는지, 컨테이너가 장애인지등)를 기록하는 표준화된 구조를 가지고 있고, 이걸 클라이언트가 이용할 수 있다.


								4. 마스터는 분산 시스템의 커널
									; Borgmaster는 처음에는 단일 프로세스로 시작했지만, 시간이 흐르면서 스케쥴러나 UI 그외 기능등을 별도의 프로세스로 분리해 내면서 스케일업하기에 좋은 구조로 발전

									: Kubernetes는 여기서 한발 더 나아가서 API 서버를 가짐 (클러스터 관리에 필요한 기능들을 마이크로 서비스로 만들어서 API 서버의 클라이언트가 되게 만들었다)


						: https://arisu1000.tistory.com/27785

					2. Omega >>  Borg 시스템을 개선시킨 시스템
						: 구글 두번째 컨테이너 관리 시스템. 
						: Omlet >> 애플리케이션을 컨테이너화하고 실행하는 작은 실행 단위.
							: 각 Omlet 은 개별적으로 구성되며, 컨테이너 클러스터 내에서 관리됨						: 클러스터에서 작업을 수행하는 데 사용되는 컨테이너. 

						: Cell >> 대규모 머신들을 관리하는 단위. 

						: CellState >> Omlets의 정보를 실시간으로 추적 
						: Cell Service >> Omlets이 필요로 하는 서비스 제공

					3. Kubernetes >> Omega, Borg 시스템을 개선한 시스템
						: 구글 세번째 컨테이너 관리 시스템. 
						: 구글의 최신 프로그래밍 언어인 Go를 기반으로 작성됨
						: 개발자가 클러스터에서 실행되는 복잡한 분산 시스템을 쉽게 배포하고 관리하기 위해 설계됨

	
	Kubernetes란? 역할은?
		: 수 많은 컨테이너를 쉽게 관리하게 해주는 시스템
			: 걍 생각해봐도 100 개의 컨테이너가 있을 때, docker container run 을 100 번 치는건 좀.. 음.. 개노답

		: 서버를 다수 운영하는 상황에서, 서로 다른 서버에서 작동하는 컨테이너를 한꺼번에 쉽게 관리 가능
 
	Kubernetes의 구조
		: 도커에 비해 좀 더 구성이 복잡
		: 구성 요소
			(1) 쿠버네티스 클러스터
				: 종류
					1. Master node 마스터 노드 
						: 개발자가 주로 통신하게 되는 노드
							: 개발자의 API 요청을 받음
						:  Master node에서 Control Plane 을 다룸. 
							: Control Plane 을 제어 가능하다는 거임 아님 Master node 의 구성 요소가 Control Plane 이라는 거임?

						: 뇌피셜 - worker node 에 접근 가능한 듯

					2. Worker node 워커 노드
						: 클라이언트가 주로 통신/이용하게 되는 노드. 
							: 인터넷을 통한 클라이언트 트래픽을 분할 받음
						: 애플리케이션이 올라감

				: CNI Container Network Interfaces 컨테이너 네트워크 인터페이스
					: 쿠버네티스 클러스터에 존재하는 컨테이너 간의 통신을 가능하게 함
						: node가 컨테이너임?

					: Master node 와 Worker node 의 유기적 통신을 가능하게 함
					: CNI 의 사용을 위해 Kubernetes network Plugin 을 제공
						:  Kubernetes network Plugin  >> 쿠버네티스에서 CNI의 사용을 가능하게 하는 플러그인. 
							: 주요 Kubernetes network Plugin  >> Flannel, calico
 								: 실습에서는 (보안이 뛰어나고 기능이 다양한) calico 를 사용

			(2) Control Plane 컨트롤 플레인 >> 쿠버네티스 클러스터 전반의 작업을 관리
				: 쿠버네티스의 작업 >>  kubectl 명령어로 master node 의 kube-apiserver 에게 API 요청을 보냄으로 써 이루어딤

				: 구성 요소 
					(1) API 서버 >> 쿠버네티스 control plane에서의 프론트앤드 역할을 함
						: 그러니까 개발자의 request 를 받는, controller 단이라는 건가?
 
					(2) ectd >> 쿠버네티스 클러스터 상의 모든 데이터를 key-value꼴로 저장하는 저장소
					(3) kube-scheduler 스케쥴러 >> 새롭게 생성되는 pod 를 어느 노드에 실행시킬지 정함
						: 쿠버네티스에서는 pod 라는 오브젝트를 통해 애플리케이션을 실행하게 됨
							: pod 는 이후 배운다
	 
					(4) 컨트롤러 매니저 >> 쿠버네티스 리소스를 관리하고 제어하는 역할
						: 컨트롤러 >> 클러스터 상태를 모니터링
							: 마스터 노드에서 실행됨
							: 컨트롤러로는 여러 종류가 있고, 각 컨트롤러는 특정 리소스 타입을 관리
								1. 디플로이먼트 Deployment 컨트롤러
								2. 서비스 Servie 컨트롤러
								3. 레플리카셋 ReplicaSet 컨트롤러


			(3) 노드
				: 구성 요소
					1. Kublet >> pod 내부의 컨테이너 실행을 담당
						: 쿠버네티스 클러스터의 각 노드에는 kublet 이 실행됨
						: pod 의 상태를 모니터링 하고, pod 상태에 이상이 있을 시 해당 pod 를 다시 배포

					2. Kube-Proxy >> node 에서 network 역할을 수행
						: node 에 존재하는 pod 들이 쿠버네티스 내부/외부와의 네트워크 토신을 가능하게 함
							: 아니 그럼 network 자체의 역할이라기 보다는 network 인터페이스의 역할을 수행한다고 말해야되는거 아님?

					3. Container runtime 컨테이너 런타임>> 컨테이너의 생명주기를 담당
						:  Container runtime interface 컨테이너 런타임 인터페이스>> Kublet 과 Container runtime 간의 통신을 가능하게 함
						: 대표적인 container runtime >> containerd, CRI-O
							: 실습에서는 가장 보편적인 containerd 를 사용

				: pod 파드 >> 컨테이너를 실행하기 위한 오브젝트
					: 쿠버네티스에서는 컨테이너가 pod 내에서 실행된다
						: 도커에서는 컨테이너가 단독으로 실행되었다
					: 각 pod 는 1개 이상의 컨테이너를 담을 수 있다
					: 그러니까 pod == 컨테이너를 그룹화 한 것
						: 도커에서는 최소 실행단위가 컨테이너 였다면, 쿠버네티스의 최소 실행 단위는 pod 라고 봄 된다
					: 다수의 pod 들은 여러 워커 노드에 분산되어 실행됨
						: 그러니까 서로 다른 pod 들은 서로 다른 노드에서 실행될 수 있지만, 하나의 pod 가 분할 되어 여러 노드에 실행되는 일은 없다는 것 
							: 다시한번 말하지만 쿠버네티스의 최소 실행단위는 pod 이다

					: 하나의 pod 에 속하는 컨테이너들은 하나의 목적을 위해 구성된 컨테이너들임. 
					: pod 는 컨테이너처럼 일시적인 존재
						: pod 는 실행할 때마다 IP주소를 배정받으므로, pod 의 IP 주소는 실행할 때마다 변경됨


			(4) 워크 로드 workload >> 쿠버네티스에서 실행되는 애플리케이션
				: 하나의 컴포넌트 형태로 실행하든, 다수의 컴포넌트가 함께 실행하든 쿠버네티스는 pod 내부에서 워크 로드를 실행하게 됨
					: 이때 pod == 실행중인 컨테이너의 집합

				: 종류
					1. Replicaset 레플리카셋>> pod 의 복제를 관리
						: 클라이언트가 요구하는 복제본 개수만큼 pod 를 복제 및 모니터링

					2. Deployment 디플로이먼트 >> 배치. 애플리케이션의 배포와 스케일링을 관리
					3. StatefulSet 스테이트풀셋 >> pod 사이에서 순서와 고유성이 보장되어야하는 경우 사용
					4. DaemonSet 데몬 셋 >> 쿠버네티스를 구성하는 모든 노드가 pod의 복사본을 실행하게 함
						: 쿠버네티스 클러스터에 새로운 노드가 추가되면 pod 역시 추가됨 
						: 주로 (로깅, 모니터링, 스토리지 와 같은) 시스템 수준의 서비스를 실행하는데 사용됨

					5. Job 과 Cronjob >> Task 가 정상적으로 완료/종료되는 것을 담당
						: pod 가 정상적으로 종료되지 않는다면 재실행시킴
						: Job >> 작업이 한 번 종료 되는 것을 담당
						: Cronjob >> 리눅스의 Crontab 과 비슷한 역할
				

 			(5) 네트워크 
				1. 서비스 service 
					: pod 를 여러개 묶어서 클러스터 외부로 노출시킬 수 있게 함
					: 장점 >> 이미 실행 중인 pod 를 외부로 노출시키기 위해 pod 내부를 수정할 필요가 없다. 쿠버네티스 서비스를 활용하면 실행중인 pod 의 수정 없이도 외부에 노출 시켜 클라이언트와의 통신이 가능하다

				2. 인그레스 Ingress 
					: 쿠버네티스 내부에 존재하는 서비스를 HTTP/HTTPS 루트를 클러스터 외부로 라우팅 하는 역할

			(6) 스토리지 
				: 컨테이너 내부 존재하는 파일들은 수명이 짧다 but 쿠버네티스 스토리지를 활용하면 pod 의 상태와 상관 없이 파일을 보관할 수 있다
					: 컨테이너 내부 존재하는 파일들은 수명이 짧다는 의미 >> 컨테이너가 삭제/재실행되면 해당 컨테이너 내부의 파일들은 모두 삭제되니까
