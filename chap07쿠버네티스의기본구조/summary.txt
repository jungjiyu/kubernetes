chap 7.  쿠버네티스의 기본 구조
	- 쿠버네티스는 도커보다 규모가 커서 좀 더 빡세다

----------------------------------------------------------------------------------------------


클러스터. 노드. 클러스터링
	: 클러스터링 >> 클러스터를 만드는 것.
	: 클러스터 >> 노드들의 집합.

	: 클러스터는 다양한 분야에서 사용되는 개념으로, 어떤 분야에서 사용되느냐 따라 (클러스터를 구성하는) "노드" 라는 용어가 의미하는 대상이 달라질 수 있다
		: ex) 네트워크 클러스터링 >>
			노드 : 네트워크 장비
			클러스터 : 네트워크 장비의 집합

		: ex) etcd 클러스터링 >>
			노드 : etcd 프로세스를 실행하는 서버
			클러스터 : (etcd 프로세스를 실행하는) 서버들의 집합



Bare Metal 베어 메탈
	: 하드웨어 상에 어떤 소프트웨어도 설치되지 않은 썡 상태
	: 베어 메탈에 운영체제가 설치 된 것 == 우리가 아는 일반적인 컴퓨터



가상화 virtualiztion >> 하나의 리소스(물리적인 하드웨어)를 논리적으로 여러 리소스로 분할하는 기술
	: 하나의 하드웨어에서 여러개의 시스템을 효율적으로 사용할 수 있게 해준다
	: 가상화에서, 실제 하드웨어 == host 이고 가상머신 == Guest 이다
	: 종류
	   : 쉽게 정리하자면 , OS 수준까지 분리 할건지 VS  host OS는 공유하면서 프로세스만 분리할 것인지
		1. 가상머신 vm 을 통한 가상화 
		    : 가상머신 >> 하이퍼바이저를 이용해, 리소스 "전체"를 가상화한 것.
			: 가상머신들은 각각 다른 OS 를 가질 수 있다.  거의 물리적인 컴퓨터와 같은 기능을 한다
			: 하이퍼바이저 hypervisor >> 가상 머신을 생성 및 활용가능하게 하는 소프트웨어
				: VM과 하드웨어간의 IO명령을 처리하는 인터페이스다 >> 생성한 가상머신의 명령어를 (물리적인) 하드웨어가 이해할 수 있도록 번역해주어, 가상머신을 사용 가능하게 해준다
					: os 별로 사용하는 명령어가 다르기 때문에, 가상머신과 하드웨어의 os 가 다르다고 할 때, 그냥 가상머신의 명령어를 그대로 (물리적인) 하드웨어에서 실행하면 해당 하드웨어는 이게 뭔 개소린지 못알아먹는다 
					: 가상머신을 생성할 때 뿐 아니라, 생성한 가상머신을 사용만 할 때에도 반드시 하이퍼바이저를 거쳐야된다.

				: 참고로  가상머신 vm 통한 가상화 방식도 세부 종류 
				    :  일반적으로 가상머신을 통한 가상화라고 하면 호스트 가상화
					(1) 호스트 가상화 >> host os 위에 올라가는 하이퍼바이저
					(2) 하이퍼바이저 가상화 >> host(하드웨어)위 에 바로 올라가는 하이퍼바이저 (host os 가 따로 없다)


			: 가상머신을 사용한 가상화를 할 경우 용량도 크고 속도도 느리다
		

		2. 컨테이너 container 를 통한 가상화
		    : 컨테이너 >> 필요한 프로세스를 표준화된 방식으로 패키징해 격리해 둔 것.
			: 프로세스만 격리한 것으로, 각 컨테이너는  host OS를 공유하게 된다.
				: host OS 를 공유한다
				== 커널을 공유한다 >> 호스트 OS 의 기능을 모두 사용 가능하다
				== 서로 다른 컨테이너끼리 통신을 주고 받을 수 있다
			: 하드웨어위에 host os 가 올라가고, 그 위에 docker 와 같은 컨테이너 응용 프로그램이 올라가고, 그 프로그램 위에 컨테이너들이 올라간다
			:  (걍 화물선에 무작위로 적재하는게 아니라) 표준화된 방식으로 패키징(컨테이너 박스)해서, 컨테이너 엔진(표준화된 운송선)만 있으면 돌아갈 수 있게하는거다.
			: 하이퍼바이저와 guest os 를 따로 필요로 하지 않아 상대적으로 가볍고 빠르다	
			: 컨테이너는 마이크로서비스아키텍쳐 MSA 의 기반이 된다. 
	: https://worlf.tistory.com/141






Container 기술
	: 왜 씀?
		1. 각 애플리케이션을 격리하여 종속성 충돌 문제를 방지
			: 베어메탈(쌩 컴퓨터. 가상머신 같은거 없는) 환경에서는 애플리케이션을 호스트 운영체제에 직접 설치하고 구성하여 애플리케이션 간 종속성 충돌 문제 발생 가능

		2. 가상화 기술을 사용하여 호스트 시스템 리소스를 효율적으로 공유 가능
			: host OS 위에 애플리케이션 프로세스의 형태로 격리한거니까

	: (Linux 운영 체제의 핵심 기능인) Linux namespaces와 cgroups를 기반으로 함
	: (컨테이너 관리를 위한) 주요 컨테이너 도구 >> Docker 등
	: Container 의 역사
		: 1979년, 컨테이너 개념 등장.  ‘chroot’ 시스템 호출의 도입.
			  chroot >> 프로세스와 그 자식 프로세스의 루트 디렉터리를 파일 시스템 내에서 새로운 위치로 변경하는 것으로, 프로세스 격리(Isolation)의 시작

		: 2000년, FreeBSD Jails가 개발
			FreeBSD Jails >> FreeBSD 시스템을 여러 독립적인 작은 “감옥(Jails)”으로 분할하고 각각에게 IP 주소를 할당

		: 2001년, Jail 메커니즘인 Linux VServer가 개발
			Linux VServer >> FreeBSD Jails와 유사한 원리로, 컴퓨터 시스템의 리소스를 분할하는 것을 가능하게함.  Linux 커널을 패치하여 구현됨

		: 2006년, 구글에서 Process Containers개발
			:  Process Containers >> 프로세스 집합의 리소스 사용량(CPU, 메모리, 디스크 I/O, 네트워크)을 제한하고 격리하기 위해 설계됨
				: 2007년에는  Process Containers가 ‘Control Groups(cgroups)’로 이름이 바뀌었으며, Linux 커널 2.6.24에 통합됨

		: 2008년, LXC 가 개발됨
			LXC (LinuX Containers) >> cgroups와 Linux namespaces를 사용하여 구현한 최초의 Linux 컨테이너 엔진으로, Linux 커널을 패치하지 않고도 동작

		: 2013년, LMCTFY(Let Me Contain That For You)가 시작
			LMCTFY(Let Me Contain That For You) >> Linux 애플리케이션 컨테이너를 제공하는 구글 컨테이너 스택의 오픈 소스 버전. 애플리케이션은 “컨테이너 인식”을 통해 하위 컨테이너를 만들고 관리 가능. 
				: 2015년부터 LMCTFY의 핵심 개념은 Open Container Foundation의 일부인 libcontainer로 이전시키기 시작됬으며, 이후 LMCTFY의 배포는 중단됨


		: 2013 년, Docker 가 등장하며 컨테이너 기술이 큰 인기를 얻음
			: 초기 Docker 는 LXC를 기반으로 했지만, 나중엔 이를 자체 라이브러리인 libcontainer 로 교체함
				: LXC 와 libcontainer 
					: LXC >> 전체 가상화 기반. 주로 서버 환경에서 사용됨. 명령줄 도구를 통해 컨테이너를 관리함
					: libcontainer >> OS 수준의 컨테이너화를 가능하게 하는 라이브러리. 주로 간단한 컨테이너 관리를 위해 Docker 와 같은 컨테이너 관리 도구에 내장되어 활용됨





쿠버네티스 Kubernetes >> 컨테이너화된 애플리케이션의 자동 배포, 확장 및 관리를 해주는 오픈소스 opensource 플랫폼
	Kubernetes 어원 
		: Kubernetes 자체는 고대 그리스어로, 배의 조타수(Helmsman)를 의미
		: Kubernetes 를 K8s 라고도 부름
			: Kubernetes단어가 K 와 s 사이에 8 글자로 이뤄져서 그렇다

	Kubernetes의 역사
	    : 요약 >>
 		: Kubernetes는 2014 년 발표된 구글의 사내 프로젝트. 
			: Borg 의 영향을 많이 받음
			: Borg >> 수천~수백 만개의 작업을 실행/관리하는 구글 사내 시스템

		: 2016 년엔 Helm이 발표됨
			: Helm 헬름>> 쿠버네티스 패키지 관리 프로그램
				: 이후 chap9에서 자세히 다룸


	    : Kubernetes는 Container mangement system  중 하나
			: https://tech.osci.kr/kubernetes-%ED%83%84%EC%83%9D-%EB%B0%B0%EA%B2%BD-%EB%B9%84%ED%95%98%EC%9D%B8%EB%93%9C-%EC%8A%A4%ED%86%A0%EB%A6%AC/

			: Container mangement system >> 컨테이너 관리 시스템
				: 구글의 대규모 데이터 센터에서 효율적 작업을 위해 개발
				; 주요 오픈소스 Container mangement system 
					1. Borg >>컨테이너를 사용하여 prod ( 장기 실행 서비스 )와 batch(non-prod)작업을 효과적으로 관리하는 시스템
						: 구글 최초의 통합 컨테이너 관리 시스템
						: Linux 컨테이너 기술 기반

						:  클러스터 Cluster 와 셀 cell <<데이터 센터 장비들의 집합 단위
							Cluster >>  군집. 한 데이터센터내에서 서로 네트워크가 연결된 장비들.
								: 1개 이상의 셸cell 로 이루어짐

							셀 cell >> 여러개의 장비들을 하나로 묶은, Cluster 를 관리하기 위한 단위로, 보통 10000 대 정도의 장비를 의미
								: Cell 에서는 여러 Task가 돌아감	
								: 보통 수천명의 사용자가 하나의 borg 셀을 공유해서 사용



						: job과 Task 
							Task >> Borg 시스템에서 실행되어야 하는 개별 작업 단위. 컨테이너에서 실행되는 리눅스 프로세스들.
								: 분류
									(1) long-running 서비스(prod) >> 오래 돌아가는, "절대" 죽으면 안되는 서비스.
									(2) batch 작업(non-prod) >> 일시적으로 돌아가는 서비스



							Job>>  작업 그룹 . 여러 TASK를 그룹화한 단위
								priority >> 우선순위. 셀 내에서 실행/대기중인 JOB의 상대적 중요도.
									: 모든 Job 엔 priority 가 있다
									: 처리 가능한 양보다 많은 Task , JOB이 셸에 할당 될 경우 쓰임

								quota 쿼터>> Job 에서 사용될 수 있는 자원의 최대치 값
									: 어떤 잡이 스케줄링 가능한지 결정하는데 쓰임
									: CPU, RAM, 디스크 등으로 표현됨
									


						: alloc	 와 alloc set 
							 : 일반적으로 task 는 alloc 를 참조, job 은 alloc set 을 참조
							alloc >> 하나 이상의 TASK 들에 대한, 장비에 미리 할당해 둔 자원들 . 
							alloc set >> 여러 alloc들을 그룹화한 단위
 			
						:  BorgMaster >> Borg 시스템의 중앙 제어 역할. 
							: 사용자가 시스템과 상호 작용할 수 있는 API를 제공하고, 작업을 예약하고 스케줄링하며 클러스터 관리를 수행
							: 구성 >> 메인 borgmaster 프로세스 + 2개의 스케쥴러 프로세스
								: 스케쥴링하는데 꽤 오래 걸리기 떄문에 빠른 처리를 위해 borgmaster의 스케쥴러는 다른 기능들과 분리되어있음
								
								:  메인 borgmaster 프로세스 
									: 역할
										1. Borg 시스템내부의 모든 객체(장비, 태스크, alloc등)의 상태를 관리.
										2. Borglet 과 통신
										3. 컨트롤러(클라이언트 request 처리)

									: 논리적으로는 단일 프로세스지만 실제로는 5개의 사본을 가짐
										: 사본 >> 셸의 상태를 메모리에 복제해서 관리함
							: 하나의 borgmaster는 셀내의 수천개의 장비를 관리 가능

							: checkpoint >> Borgmaster의 특정 시점에서의 상태
								: paxos 저장소에 기간별 스냅샷으로 보관됨


						: 스케쥴러 scheduler >> Job의 자원조건 보고, 적당한 자원을 가진 장비에 Task를 할당
							: 작동 절차
								1. Job 이 제출되면 Borgmaster는 그걸 paxos 저장소에 저장, Job의 Task를 pending 큐에 추가
								2. scheduler가 pending 큐를 비동기적으로 확인해 적절한 장비에 Task 를 할당

							: 주로 Task 를 관리하는거지, Job을 관리하는게 아니다
							: 높은 우선순위를 가진 Task 가 먼저 스케쥴링 된다


						: Borglet >> 셀내의 모든 장비들에서 돌아가는 Borg 에이전트
							: 역할 
								1. borgmaster와 해당 장비가 통신 가능하게 함											:  borgmaster나 모니터링 시스템에 장비 상태 보고
		
								2. 태스크의 시작/정지. 실패했을때 재시작등을 담당
								3. 로컬자원 관리
							: borgmaster에 요청이 몰리는걸 막기 위해, borgmaster에서 몇 초 간격으로 borglet에서 장비의 상태를 요청한다
								: 이거 SMTP Simple Mail Transfer Protocol아님?


						: Borg 와 Kubernetes 
							: Borg 의 단점 
								1. Job은 Task 를 그룹화 하는 용도로만 사용 가능
									: Borg에선 멀티 Job을 하나로 처리하거나, 잡의 특정 하위집합만 묶어서 처리할 수 없어서 롤링 업데이트나 잡 크기변경등을 못한다.
									: Kubernetes는 label을 이용해서 pods들을 스케쥴링한다.  작업할 대상을 정한다.


								2. 장비당 하나의 IP 주소만 있어서 처리하기가 복잡
									: borg에서는 장비에 IP가 하나밖에 없어서 각 Task들이 장비의 포트를 구분해서 사용해야됨. 
										: 포트를 자원으로 간주하고 관리하는게 복잡했음 
											: 태스크가 얼마나 많은 포트를 사용할 지 등을 미리 정해놔야됨
									: Kubernetes는 모든 pod 와 서비스들이 각각의 IP 주소를 가짐. 포트를 관리할 필요가 없음


							: Borg 의 장점
								1. alloc은 유용
									: borg에서는 alloc을 이용해서 부가 서비스(웹서버가 사용하는 데이터를 주기적으로 업데이트하는 등)들을 별도의 팀에서 개발하는게 가능
									: kubernetes 에서는 alloc 와 유사한 개념으로 pod 가 있음(같은 pod내에 헬퍼 컨테이너를 사용가능)

								2. 클러스터 관리라는게 Task 관리만 하는건 아니다
									: borg에선 Task와 장비의 생명주기를 관리하는 것 뿐 아니라 네이밍, 로드밸런싱등의 서비스 제공
									: Kubernetes는 service를 이용해서 네이밍과 로드밸런싱을 지원
										: 네이밍 >> label selector를 통해서 pod에 동적으로 서비스 이름을 정의 가능. 그리고 정의된 클러스터내의 모든 컨테이너들은 서비스명을 기반으로 서비스에 연결 가능
										: 로드 밸런싱 >>  자동으로 서비스에 대한 커넥션들을 pod에 로드밸런싱

								3. Introspection이 중요하다
									: borg 에선 디버깅 정보를 모든 사용자가 볼 수 있다. 
									: Kubernetes는 borg의 introspection 테크닉을 많이 가져옴(
자원 모니터링을 위해 cAdvisor를 사용하고, Elasticsearch/Kibana/Fluentd 를 이용해 로그를 수집.  ) 마스터는 객체의 상태에 대한 스냅샷을 요청가능.  모든 구성요소들이 사용할 수 있는 이벤트(pod가 스케쥴되었는지, 컨테이너가 장애인지등)를 기록하는 표준화된 구조를 가지고 있고, 이걸 클라이언트가 이용할 수 있다.


								4. 마스터는 분산 시스템의 커널
									; Borgmaster는 처음에는 단일 프로세스로 시작했지만, 시간이 흐르면서 스케쥴러나 UI 그외 기능등을 별도의 프로세스로 분리해 내면서 스케일업하기에 좋은 구조로 발전

									: Kubernetes는 여기서 한발 더 나아가서 API 서버를 가짐 (클러스터 관리에 필요한 기능들을 마이크로 서비스로 만들어서 API 서버의 클라이언트가 되게 만들었다)


						: https://arisu1000.tistory.com/27785

					2. Omega >>  Borg 시스템을 개선시킨 시스템
						: 구글 두번째 컨테이너 관리 시스템. 
						: Omlet >> 애플리케이션을 컨테이너화하고 실행하는 작은 실행 단위.
							: 각 Omlet 은 개별적으로 구성되며, 컨테이너 클러스터 내에서 관리됨						: 클러스터에서 작업을 수행하는 데 사용되는 컨테이너. 

						: Cell >> 대규모 머신들을 관리하는 단위. 

						: CellState >> Omlets의 정보를 실시간으로 추적 
						: Cell Service >> Omlets이 필요로 하는 서비스 제공

					3. Kubernetes >> Omega, Borg 시스템을 개선한 시스템
						: 구글 세번째 컨테이너 관리 시스템. 
						: 구글의 최신 프로그래밍 언어인 Go를 기반으로 작성됨
						: 개발자가 클러스터에서 실행되는 복잡한 분산 시스템을 쉽게 배포하고 관리하기 위해 설계됨

	
	Kubernetes란? 역할은?
		: 수 많은 컨테이너를 쉽게 관리하게 해주는 시스템.  복잡한 애플리케이션의 운영을 자동화가능
			: 걍 생각해봐도 100 개의 컨테이너가 있을 때, docker container run 을 100 번 치는건 좀.. 음.. 개노답

		: 서버를 다수 운영하는 상황에서, 서로 다른 서버에서 작동하는 컨테이너를 한꺼번에 쉽게 관리 가능
		: 클라우드 환경에서 유용하게 사용됨
 		: 계속해서 Desired State (원하는 상태)를 만들기 위해 Current State(현재 상태)를 바꾸는 플랫폼
			: 그러니까 컨테이너/네트워크 .. 등에 대한 desired state 를 쿠버네티스에게 알려주면 쿠버네티스는 계속해서 current state를 체크한다
 

	Kubernetes의 구조
		: 도커에 비해 좀 더 구성이 복잡
		: 구성 요소<<  교재 +구글링 통합 정리 
			: https://ooeunz.tistory.com/118
			: https://velog.io/@youknowwhat/Kubernetes-%EA%B8%B0%EB%B3%B8-%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8
			: 클러스터 >> 같은 목적으로 묶인, 여러 대의 컴퓨터 집합

			: Label 과 Selector 
				: Label 레이블 >>   Object를 구분/관리하기 쉬우라고 사용자가 할당하는 특정값
					: 꼬리표 같은 느낌
					: Key-Value 형식
						: 그러니까 yaml 파일에서 "어쩌구항목 :저쩌구값 " 으로 기술된다는 거임
						: 사용가능한 key명이 따로 정해져있는건 아니고 (해당 리소스 내에서 고유하다면) 맘대로 정의 가능

					: 모든 Object 에는 label 을 태그 가능(=달 수 있다) 
						: 주로 pod 에 태그됨

					: label 은 수정이 가능하다
					: how to use
						: label 정의 >> pod 를 생성할 때, metadata 아래의 labels 부분에,  이 pod 에 대한 고유한 라벨 정의 
						   : 그러니까 아래와 같은 형식으로 정의
							apiVersion: v1
							kind: Pod

							metadata:
							  name: Pod이름
							  labels:
							    키명1: 어떤값
							    키명2: 어떤값
								.
								.
								.

						: label 활용 >> service 를 생성할 떄, spec아래의 selector 부분에 특정 label 을 입력하여 생성
						    : selector를 통해 key:value로 해당 내용과 매칭되는 레이블이 있는 파드와 연결되는거다

						    : 그러니까 아래와 같은 형식으로 정의
							apiVersion: v1
							kind: Service

							metadata:
								.
								.
								.

							spec :
							  selector: 
							    키명1: 어떤값
								.
								.
								.


						: pod 에 부착된 label 목록 확인 >>  
							(1) kubectl get pod --show-labels >> 모든 label 에 대한 pod 목록 확인
							(1) kubectl get pod  -L 키1,키2,키3..>> 특정 label들에 대한 pod 목록 확인




				: Label Selector >> Label 을 이용해 특정 리소스를 선택(필터링)하여 원하는 작업을 수행
					: selector 의 선택 기준
						

			: Resource 와 Object
				: Pod 의 경우, 리소스로써의 Pod 도 있고 Object 로써의 Pod 도 있다
					: 리소스 Pod >> 
					: Object Pod >> 


				: Resource 리소스>> 쿠버네티스에서 관리하는 "추상적인" 엔터티. Object 를 생성하기 위한 템플릿/정의

					: 쿠버네티스에서는 「리소스」를 등록하는 것으로 컨테이너의 실행과 로드밸런서의 작성을 비동기로 실행

					:  리소스의 정의 >> YAML 또는 JSON 파일로


					: 종류 	
					*******(1) Workloads 리소스 >> 컨테이너의 실행에 관한 리소스
							1. Pod >> 컨테이너 그룹을 실행하는 리소스
								: 가장 기본적인 실행 단위
							2. Replicaset 레플리카셋 >>특정 갯수로 pod의 복제본을 유지
								: 클라이언트가 요구하는 복제본 개수만큼 pod 를 복제 및 모니터링

							3. Deployment 디플로이먼트 >> 배치. 애플리케이션의 배포와 스케일링을 관리
							4. DaemonSet 데몬 셋 >> 쿠버네티스의 모든 노드가 pod의 복사본을 실행하게 함
								: 쿠버네티스 클러스터에 새로운 노드가 추가되면 pod 역시 추가됨 
								: 주로 (로깅, 모니터링, 스토리지 와 같은) 시스템 수준의 서비스를 실행하는데 사용됨
							5. StatefulSet 스테이트풀셋 >> pod 사이에서 순서와 고유성이 보장되어야하는 경우 사용
							6.  Job 과 Cronjob >> Task 가 정상적으로 완료/종료되는 것을 담당
								: pod 가 정상적으로 종료되지 않는다면 재실행시킴
								: Job >> 작업이 한 번 종료 되는 것을 담당
								: Cronjob >> 리눅스의 Crontab 과 비슷한 역할




				



						(2) Discovery & Load Balance 리소스 >> 컨테이너를 외부에 노출하기 위한 엔드 포인트를 제공하는 리소스. 네트워크 관련 리소스.

							1. Service 서비스 >> 클러스터 내에서 실행 중인 Pod 집합에 대한 네트워크 접근을 제공하는 리소스
								: 기능
									0. 외부 접근 >> 클러스터 외부에서 내부 파드에 접근할 수 있도록 함
										 :Service를 통해 실행중인 pod 의 수정 없이도 외부에 노출 시켜 클라이언트와의 통신이 가능해진다

									1. 서비스 디스커버리 >> 클러스터 내에서 Pod 간의 통신을 용이하게 함
										:   파드의 IP 주소 대신 하나의 고정된 IP 주소와 DNS 이름을 제공.
											: 파드가 생성되거나 삭제되어 IP 주소가 변경되더라도, Service를 통해 안정적인 네트워크 연결을 유지할 수 있다.


									2. 로드 밸런싱>> 다수의 Pod 에 대한 트래픽을 자동으로 분산시켜 과도한 부하 방지
										: 쿠버네티스는 이를 위해 ClusterIP와 같은 내부 IP를 할당하고, 해당 IP를 통해 트래픽을 관리



								: 타입
									(1) ClusterIP : 클러스터 내부에서만 접근 가능한 IP 주소를 제공
									(2) NodePort : 클러스터의 각 노드에서 고정된 포트를 열어 외부에서의 접근을 허용
									(3) LoadBalancer : 클라우드 환경에서 외부 로드 밸런서를 생성하여, 외부에서 직접 서비스에 접근할 수 있게 함
									(4) Headless (None)
									(5) ExternalName : DNS 이름을 외부 서비스로 매핑하여 클러스터 내에서 외부 서비스에 접근할 수 있게 함


							2. Ingress 인그레스 >> 클러스터 외부에서 내부 서비스로의 HTTP(S) 트래픽을 제어하고 관리하는 리소스	
								: Service 보다 더 높은 수준의 네트워크 리소스 
									: Service 리소스를 통해 트래픽을 전달
										: Service 리소스를 대상으로 동작하며, 여러 Service로의 트래픽을 라우팅
									:  Service 보다 더 높은 수준 == Ingress가 Service보다 더 세부적이고 복잡한 네트워크 트래픽 관리 기능
										: Service 는 기본이되는 네트워크 리소스
										: Ingress는 Service 위에서 동작하는 좀 더 세부적인, 정교한 네트워크 리소스


								: 기능
									0. HTTP(S) 라우팅 >> 클러스터 외부에서 내부 파드에 접근할 수 있도록 함
										: URL 경로나 호스트 이름 기반의 외부에서 클러스터로 들어오는 HTTP(s) 트래픽을, 적절한 서비스로 라우팅 시킨다
										: 단일 외부 IP 주소를 사용해 여러 서비스에 대한 요청을 관리 가능

									1. TLS/SSL 제공 >> 보안 강화

									2. 리버스 프록시 역할 >> 사용자가 지정한 규칙에 따라 트래픽을 각 서비스로 전달 가능
										: Ingress는 여러 서비스에 대한 접근을 중앙 집중적으로 관리

								: Ingress 컨트롤러 
									: Ingress 리소스는 직접적으로 트래픽을 관리하지 않음. Ingress 컨트롤러에 의해 트래픽의 관리가 수행됨
									: Ingress 리소스에 정의된 규칙에 따라 Ingress 컨트롤러가 트래픽을 적절하게 라우팅한다.




						(3) Config & Storage 리소스 >> 설정/기밀정보/Persistent Volume 등에 관한 리소스

							1. Secret
							2. ConfigMap
							3. PersistentVolumeClaim


						(4) Cluster 리소스 >> 클러스터 자체의 행동을 정의하는 리소스
							: 보안 설정과 정책, 클러스터의 관리성을 높임
							1. Node
							2. Namespace
							3. PersistentVolume
							4. ResourceQuota
							5. ServiceAccount
							6. Role
							7. ClusterRole
							8. RoleBinding
							9. ClusterRoleBinding
							10. NetworkPolicy


						(5) Cluster 리소스 >>	클러스터 내의 다른 리소스를 조작하기 위한 리소스
							1. LimitRange
							2. HorizontalPodAutoScalar
							3. PodDisruptionBudget
							4. CustomResourceDefinition


				: https://somaz.tistory.com/198
				: https://thecodingmachine.tistory.com/9


				: Object >> 쿠버네티스 클러스터의 상태 관리를 위한 객체.  resouce를 기반으로 생성된 "실제" 인스턴스
					: https://velog.io/@youknowwhat/Kubernetes-%EA%B8%B0%EB%B3%B8-%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8

					: 즉 쿠버네티스 시스템의 도메인을 모델링 한 것 
					: 영속성을 가진다
						: 영속성 persistence >> 프로그램이 종료되도, 사라지지 않는 데이터 특성
					: 생성 의도를 가지고 있다
						: Desired State가 기술된 Object를 생성하면 쿠버네티스 시스템이 해당 Object의 Desired State를 보장하는 방식으로 동작한다

					: Object 정의 >> yaml 이나 json 같은 파일로  (요즘엔 거의 yaml)

					: 종류
						(1)  Basic Object 기본 오브젝트>> 쿠버네티스에서의 가장 기본적인 Object
							1. Pod >>  쿠버네티스에서 생성/관리 가능한 작은 컴퓨팅 단위

								: 독립적인 공간과 IP를 갖는다. 
								    : IP 
									: 파드가 생성될 때 고유의 IP 주소가 할당됨
									: 가변적이다
										: 파드 삭제 및 재생성 시, IP 주소는 바뀐다
										: 이로 인한 문제 해결 방안으로 Service 라는 Object 가 존재한다
									: 쿠버네티스 클러스터 내에서만 해당 IP를 통해 해당 파드로 접근이 가능하고 외부에서는 해당 IP로 접근할 수 없다.

									
								:  쿠버네티스에서는 컨테이너가 pod 내에서 실행된다
									: 컨테이너 
										: 하나의 독립적인 서비스/기능을 구동 가능 
	
										: 서비스(컨테이너)끼리 연결/통신될 수 있도록 포트를 가진다
											: 한 컨테이너가 포트를 하나 이상 가질 수 있지만, 동일한 파드에 속한 컨테이너끼리 포트가 중복될 순 없다
											: 여러 기능을 묶어 하나의 목적으로 배포해서 사용 가능

									: 하나의 파드는 1개 이상의 컨테이너를 가진다
									: 그러니까 pod == 컨테이너를 그룹화 한 것
										: 도커에서는 최소 실행단위가 컨테이너 였다면(도커에서는 컨테이너가 단독으로 실행됬었다), 쿠버네티스의 최소 실행 단위는 pod 라고 봄 된다
									: 하나의 pod 에 속하는 컨테이너들은 하나의 목적을 위해 구성된 컨테이너들.


									: 컨테이너들을 파드라는 하나의 호스트로 묶어 관리한다고 보면된다
										: 그래서 localhost를 이용해 한 컨테이너에서 다른 컨테이너로의 접근이 가능하다



								: 다수의 pod 들은 여러 worker node 에 분산되어 실행됨
									: 그러니까 서로 다른 pod 들은 서로 다른 노드에서 실행될 수 있지만, 하나의 pod 가 분할 되어 여러 노드에 실행되는 일은 없다 ( 쿠버네티스의 최소 실행단위는 pod)










							2. Service >> Pod 접속을 안정적으로 유지하기 위한 기능을 제공하는 오브젝트
								: Pod 는 언제든 죽거나 재생성될 수 있는 유동적인 오브젝트임에도 안정적인/고정적인 접속을 가능하게 해준다
								: Pod 와는 달리 사용자가 직접 지우지 않는 한 삭제/재생성되지 않으므로 고정된 접속 정보를 가져 안정적
								: 하나의 service는 여러개의 Pod와 연결될 수 있다
									: 여러개의 Pod 와 연결되는 경우 Serive 가 트래픽을 분산하여 Pod 에게 전달해준다. 
										: 그러니까 (service 오브젝트의 spec 정의 부분의) selector 를 통한 로드밸런싱이 가능

								: 타입
									(1) ClusterIP : 클러스터 내부에서만 사용 가능한 IP 주소를 제공
										: Service 의 디폴트 타입
										: 외부에서는 ClusterIP를 사용한 접근이 불가하지만, 클러스터 내부에서는 "ClusterIP:서비스포트" 의 형식으로 접근이 가능

									(2) NodePort : 클러스터의 각 노드에서 고정된 포트를 열어 외부에서의 접근을 허용
										: 외부에서 노드 IP의 특정 포트로 들어오는 ( "NodeIP:NodePort" 형식의 ) request를 감지하여, 연결된 Pod의 해당 port로 트래픽을 전달
										: NodePort 타입으로 만들어도 기본적으론 ClusterIP가 할당된다	
											:  그러니까 Nodeport 타입은 ClusterIP 타입의 기능들을 포함하고 있다. 
										: 특정 NodePort 서비스와 연결되어 있는 Pod가 할당되어 있는 노드들에는, 같은 Service 들이 자동으로 생성되어 NodeIP가 달라도 같은 NodePort로 해당 파드들에게 접근이 가능하다
											: 즉, 어느 노드이건 간에 같은 NodePort로 접속하면 해당 NodePort로 열려있는 서비스에 연결되고 해당 서비스는 Pod 에 트래픽을 전달해준다. 
											: 같은 노드 내의 Pod 들에만 접근 가능하게 하고 싶다면 externalTrafficPolicy 속성을 Local로 설정하면 된다. 그렇게 되면 특정 NodePort의 IP로 접근하는 트래픽은 해당 노드에 올려져있는 파드에만 전달된다.

										: 30000~32767 사이에서 할당이 가능하며, 지정하지 않으면 해당 범위에서 자동으로 할당됨




									(3) LoadBalancer : 클라우드 환경에서 외부 로드 밸런서를 생성하여, 외부에서 직접 서비스에 접근할 수 있게 함
									(4) Headless (None)
									(5) ExternalName : DNS 이름을 외부 서비스로 매핑하여 클러스터 내에서 외부 서비스에 접근할 수 있게 함

							3. Volume >> 				
								: 컨테이너 내부 존재하는 파일들은 수명이 짧다 but 쿠버네티스 스토리지를 활용하면 pod 의 상태와 상관 없이 파일을 보관할 수 있다
									: 컨테이너 내부 존재하는 파일들은 수명이 짧다는 의미 >> 컨테이너가 삭제/재실행되면 해당 컨테이너 내부의 파일들은 모두 삭제되니까



							4. Namespace 

	
						(2) 그외의 추가적인 오브젝트들 >> ConfigMap, Secret, ResourceQuota , LimitRange
	

			:  controller >> "특정 타입의 리소스"의 "상태"를 (사용자의 의도대로 작동하도록) 제어/관리. 
				: 특정 리소스의 상태를 지속적으로 감시하고, 클러스터의 현재 상태와 사용자가 원하는 상태 간의 차이를 조정한다
					ex) 사용자가 "레플리카셋(ReplicaSet)을 통해 3개의 파드를 유지하고 싶다"고 지정한 경우  -> 레플리카셋 컨트롤러는 실제로 3개의 파드가 실행되고 있는지 확인하고, 만약 3개보다 적거나 많다면 이를 조정


		
				: 스프링 부트의 컨트롤러( url 매핑 따라 메서드 실행)랑은 다른 개념
					: kubernetes에서의 컨트롤러는, 말 그대로 (클러스터의 리소스를) "컨트롤" 한다는 의미에서 컨트롤러라고 이름이 붙여진 거임

				: 주요 컨트롤러
					1. 디플로이먼트 Deployment 컨트롤러 >> Deployment 리소스를 관리하는 컨트롤러
					2. 서비스 Service 컨트롤러 >> servie 리소스를 관리하는 컨트롤러
					3. 레플리카셋 ReplicaSet 컨트롤러 >> 지정된 수의 파드가 항상 실행되도록 관리


				: 컨트롤러는 API 서버와 상호 작용하여 작동
					:  그러니까 컨트롤러가 (클러스터의 상태를 모니터링하고 조정하는 데) 필요한 모든 정보를 API 서버를 통해 얻고, 조정 작업도 API 서버를 통해 수행한다


			: 워크 로드 workload >> 쿠버네티스에서 실행되는 애플리케이션
				:  쿠버네티스는 pod 내부에서 workload를 실행하게 됨


	
			: Master Node 마스터 노드 >> 클러스터 전체를 관리하는 컨트롤러
				: Master Node 의 컴포넌트(구성 요소)
					(1) Control Plane 컨트롤 플레인 >> 쿠버네티스 클러스터 전반의 작업을 관리하는 , 쿠버네티스 클러스터의 두뇌 같은 부분
						: Control Plane 의 구성 요소 
							:  Control Plane 은 Master Node 의 일부 컴포넌트 집합을 지칭하는 거라고 봄 된다.
							1.  kube-apiserver >>   API 서버. control plane에서의 프론트앤드 역할을 함
								: 그러니까 클라이언트(사용자, 애플리케이션, 컴포넌트 등)로부터의 request을 받아들이고, 이를 적절한 컴포넌트에게 전달해준다
									: 브라우저와 같은 클라이언트 뿐 아니라 클러스터의 컴포넌트들 간의 정보 교환도 얘가 중재한다는 것.

									: 즉, API 서버는 쿠버네티스의 중앙 통신 허브
										: 쿠버네티스에서 (kubectl 명령어와 같은) 모든 인터랙션이 이 API 서버를 통해 처리된다
											: ex) 컨트롤러는 API 서버에 주기적으로 request을 보내어, 자신이 관리하는 리소스의 현재 상태를 확인

								: 여러 인스턴스를 한번에 실행 가능
								: 부하 분산을 통한 트래픽 관리 가능


 							2.  ectd 엣시디 >> 클러스터의 모든 상태 데이터를 key-value 형태로 저장하는 데이터베이스 역할
								: 이를 통해 클러스터의 상태를 특정 시점으로 되돌릴 수 있다
								: etcd 는  etc 디렉터리(/etc) 와 distributed (터뜨리다) 의 합성어
								: 보통 etcd는 클러스터링 되어있다				
									: etcd 클러스터링 에서의 노드 >> etcd 프로세스가 실행되는 "인스턴스"
										: 인스턴스 >> 서버 또는 가상 머신
											: API server 와는 별개인, Master Node 를 구성하는 또다른 서버를 의미.
												: (Master Node 의 일부 컴포넌트 집합을 Control plane 이라고 하는거지) Master Node 가 Control Plane 으로만 이뤄진게 아님을 기억하자. (Control Plane 에 서버는 API 서버밖에 없지만, 그렇다고 Master Node 를 구성하는 서버가 API 서버밖에 없다는 게 아니라는 것)
										: 노드라고 worker node 를 의미하는거 아님 주의

									: 그러니까 etcd 가 클러스팅 되어있다 == etcd라는 분산 키-값 저장소는 여러 개의 서버에 걸쳐 분산되어 실행된다
									: etcd 클러스터는 최소 3개의 노드로 구성되는 것이 일반적
										: 5개 또는 그 이상의 노드로 구성할 수도 있음
										: 주로 홀수 개로 하는데, 이 이유 >> quorum 알고리즘을 사용하여 데이터의 일관성을 보장하기 떄문
  
									: 왜 서버 하나에서 etcd 를 돌리는게 아니라 여러개로 분산해서 실행시킬까>> 클러스터의 장애 복구를 위해 다중 인스턴스로, 데이터의 복제를 통해 고가용성을 유지하기 위함




							3. kube-controller-manager >> 컨트롤러 매니저 . 여러 controller들을 관리/실행하는 역할. 
								: 컨트롤러 매니저는 여러 controller 들을 단일 프로세스로 관리
									: 즉, 여러 컨트롤러들 각각은 논리적으로 개별 프로세스지만(=각각 다른 리소스를 관리하니까) , 모두 kube-controller-manager라는 하나의 프로세스에서 실행된다
									: 여러 컨트롤러 작업들이, 하나의 프로세스에서 동시다발적으로 작동한다

									: 왜 단일 프로세스로 관리? >> 그게 더 효율적이니까...
										: 각 컨트롤러를 독립된 프로세스로 실행하는 것보다 단일 프로세스로 여러 컨트롤러를 실행하는게 리소스를 더 효율적으로 사용가능
										: 하나의 프로세스만 모니터링/관리하면되니까 관리 및 유지보수가 더 쉬움
										: 단일 프로레스로 관리하면 컨트롤러 간 통신이 더 원활


								: 그러니까 controller 는 리소스를 컨트롤하고, contoller manager 는 이러한 controller 를 컨트롤한다는 것



							4. kube-scheduler >> 스케쥴러. 새롭게 생성된 Pod를 적절한 Node에 할당하는 자동화된 스케줄링 프로세스를 담당.
								: 클러스터의 효율적인 리소스 사용과 파드의 성능, 가용성을 유지를 가능하게 함


								: 스케줄링 >> Pod 를 특정 Node 에 배치하는 것	
									: Pod 가 생성될 때마다 자동으로 수행됨
									: 과정
										(1) 적합한 노드 집합 필터링 >> (파드가 요구하는 특정 리소스( CPU, 메모리.. )나 노드에 설정된 제약 조건등을 고려해) 모든 노드 중에서 파드를 실행할 수 있는 적합한 노드 집합을 필터링
										(2) 점수 매기기 Scoring >> (구한 노드 집합에서) 각 노드에 ( 노드의 리소스 사용률, 파드 간의 친화성 및 반감성 규칙, 네트워크 토폴로지 등을 기준으로) 점수를 매김	
										(3) 최적의 노드 선택 >> 점수가 가장 높은 노드를 선택해 해당 노드에 파드를 배치.

									: 여러가지 알고리즘(전략)이 사용됨
										: "최소화된 리소스 사용", "균등한 로드 분산", "고가용성 유지" 등
								: Affinity 와 Anti-affinity 
									: 친화성 Affinity >>  Pod를 서로 가까이 배치하는  규칙
										:  특정 노드에 파드를 선호하게 함

									: 반감성 Anti-affinity >> Pod를 서로 멀리 배치하는  규칙
										: 특정 노드나 동일한 노드 그룹에 파드가 배치되지 않도록 함

								: 사용자 정의 우선순위와 선호도 규칙을 사용하여 특정 Node가 Pod 를 더/덜 선호하게 할 수 있다 




			: Worker Node 노드 >> 컨테이너가 배포되는 물리적인 머신



		: 구성 요소 << 교재 정리 
			(1) 쿠버네티스 클러스터
				: 종류
					1. Master node 마스터 노드 
						: 개발자가 주로 통신하게 되는 노드
							: 개발자의 API 요청을 받음
						:  Master node에서 Control Plane 을 다룸. 
							: Control Plane 을 제어 가능하다는 거임 아님 Master node 의 구성 요소가 Control Plane 이라는 거임?

						: 뇌피셜 - worker node 에 접근 가능한 듯

					2. Worker node 워커 노드
						: 클라이언트가 주로 통신/이용하게 되는 노드. 
							: 인터넷을 통한 클라이언트 트래픽을 분할 받음
						: 애플리케이션이 올라감

				: CNI Container Network Interfaces 컨테이너 네트워크 인터페이스
					: 쿠버네티스 클러스터에 존재하는 컨테이너 간의 통신을 가능하게 함
						: node가 컨테이너임?

					: Master node 와 Worker node 의 유기적 통신을 가능하게 함
					: CNI 의 사용을 위해 Kubernetes network Plugin 을 제공
						:  Kubernetes network Plugin  >> 쿠버네티스에서 CNI의 사용을 가능하게 하는 플러그인. 
							: 주요 Kubernetes network Plugin  >> Flannel, calico
 								: 실습에서는 (보안이 뛰어나고 기능이 다양한) calico 를 사용

			(2) Control Plane 컨트롤 플레인 >> 쿠버네티스 클러스터 전반의 작업을 관리
				: 쿠버네티스의 작업 >>  kubectl 명령어로 master node 의 kube-apiserver 에게 API 요청을 보냄으로 써 이루어딤

				: 구성 요소 
					(1) API 서버 >> 쿠버네티스 control plane에서의 프론트앤드 역할을 함
						: 그러니까 개발자의 request 를 받는, controller 단이라는 건가?
 
					(2) ectd >> 쿠버네티스 클러스터 상의 모든 데이터를 key-value꼴로 저장하는 저장소
					(3) kube-scheduler 스케쥴러 >> 새롭게 생성되는 pod 를 어느 노드에 실행시킬지 정함
						: 쿠버네티스에서는 pod 라는 오브젝트를 통해 애플리케이션을 실행하게 됨
							: pod 는 이후 배운다
	 
					(4) 컨트롤러 매니저 >> 쿠버네티스 리소스를 관리하고 제어하는 역할
						: 컨트롤러 >> 클러스터 상태를 모니터링
							: 마스터 노드에서 실행됨
							: 컨트롤러로는 여러 종류가 있고, 각 컨트롤러는 특정 리소스 타입을 관리
								1. 디플로이먼트 Deployment 컨트롤러
								2. 서비스 Servie 컨트롤러
								3. 레플리카셋 ReplicaSet 컨트롤러


			(3) 노드
				: 구성 요소
					1. Kublet >> pod 내부의 컨테이너 실행을 담당
						: 쿠버네티스 클러스터의 각 노드에는 kublet 이 실행됨
						: pod 의 상태를 모니터링 하고, pod 상태에 이상이 있을 시 해당 pod 를 다시 배포

					2. Kube-Proxy >> node 에서 network 역할을 수행
						: node 에 존재하는 pod 들이 쿠버네티스 내부/외부와의 네트워크 토신을 가능하게 함
							: 아니 그럼 network 자체의 역할이라기 보다는 network 인터페이스의 역할을 수행한다고 말해야되는거 아님?

					3. Container runtime 컨테이너 런타임>> 컨테이너의 생명주기를 담당
						:  Container runtime interface 컨테이너 런타임 인터페이스>> Kublet 과 Container runtime 간의 통신을 가능하게 함
						: 대표적인 container runtime >> containerd, CRI-O
							: 실습에서는 가장 보편적인 containerd 를 사용

				: pod 파드 >> 컨테이너를 실행하기 위한 오브젝트
					: 쿠버네티스에서는 컨테이너가 pod 내에서 실행된다
						: 도커에서는 컨테이너가 단독으로 실행되었다
					: 각 pod 는 1개 이상의 컨테이너를 담을 수 있다
					: 그러니까 pod == 컨테이너를 그룹화 한 것
						: 도커에서는 최소 실행단위가 컨테이너 였다면, 쿠버네티스의 최소 실행 단위는 pod 라고 봄 된다
					: 다수의 pod 들은 여러 워커 노드에 분산되어 실행됨
						: 그러니까 서로 다른 pod 들은 서로 다른 노드에서 실행될 수 있지만, 하나의 pod 가 분할 되어 여러 노드에 실행되는 일은 없다는 것 
							: 다시한번 말하지만 쿠버네티스의 최소 실행단위는 pod 이다

					: 하나의 pod 에 속하는 컨테이너들은 하나의 목적을 위해 구성된 컨테이너들임. 
					: pod 는 컨테이너처럼 일시적인 존재
						: pod 는 실행할 때마다 IP주소를 배정받으므로, pod 의 IP 주소는 실행할 때마다 변경됨


			(4) 워크 로드 workload >> 쿠버네티스에서 실행되는 애플리케이션
				: 하나의 컴포넌트 형태로 실행하든, 다수의 컴포넌트가 함께 실행하든 쿠버네티스는 pod 내부에서 워크 로드를 실행하게 됨
					: 이때 pod == 실행중인 컨테이너의 집합

				: 종류
					1. Replicaset 레플리카셋>> pod 의 복제를 관리
						: 클라이언트가 요구하는 복제본 개수만큼 pod 를 복제 및 모니터링

					2. Deployment 디플로이먼트 >> 배치. 애플리케이션의 배포와 스케일링을 관리
					3. StatefulSet 스테이트풀셋 >> pod 사이에서 순서와 고유성이 보장되어야하는 경우 사용
					4. DaemonSet 데몬 셋 >> 쿠버네티스를 구성하는 모든 노드가 pod의 복사본을 실행하게 함
						: 쿠버네티스 클러스터에 새로운 노드가 추가되면 pod 역시 추가됨 
						: 주로 (로깅, 모니터링, 스토리지 와 같은) 시스템 수준의 서비스를 실행하는데 사용됨

					5. Job 과 Cronjob >> Task 가 정상적으로 완료/종료되는 것을 담당
						: pod 가 정상적으로 종료되지 않는다면 재실행시킴
						: Job >> 작업이 한 번 종료 되는 것을 담당
						: Cronjob >> 리눅스의 Crontab 과 비슷한 역할
				

 			(5) 네트워크 
				1. 서비스 service 
					: pod 를 여러개 묶어서 클러스터 외부로 노출시킬 수 있게 함
					: 장점 >> 이미 실행 중인 pod 를 외부로 노출시키기 위해 pod 내부를 수정할 필요가 없다. 쿠버네티스 서비스를 활용하면 실행중인 pod 의 수정 없이도 외부에 노출 시켜 클라이언트와의 통신이 가능하다

				2. 인그레스 Ingress 
					: 쿠버네티스 내부에 존재하는 서비스를 HTTP/HTTPS 루트를 클러스터 외부로 라우팅 하는 역할

			(6) 스토리지 
				: 컨테이너 내부 존재하는 파일들은 수명이 짧다 but 쿠버네티스 스토리지를 활용하면 pod 의 상태와 상관 없이 파일을 보관할 수 있다
					: 컨테이너 내부 존재하는 파일들은 수명이 짧다는 의미 >> 컨테이너가 삭제/재실행되면 해당 컨테이너 내부의 파일들은 모두 삭제되니까




	Kubernetes Network  << TODO
		: https://medium.com/finda-tech/kubernetes-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EC%A0%95%EB%A6%AC-fccd4fd0ae6


